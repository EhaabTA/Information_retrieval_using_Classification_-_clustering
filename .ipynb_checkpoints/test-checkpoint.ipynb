{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ae8bc6f-2e8e-47e2-abe8-a9287d9a655c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter Query(Type -1 to exit):  transformer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(21, 0.38735031509185675), (18, 0.2048209890122228)]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter Query(Type -1 to exit):  -1\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import nltk\n",
    "import os\n",
    "import string\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "import math \n",
    "Dictionary = {} #Create a global dictionary\n",
    "DocVectors = {} #Create a global dictionary for Document Vectors\n",
    "\n",
    "\n",
    "def FileRead(): \n",
    "    Folder = 'ResearchPapers'\n",
    "    Pattern = '*.txt' \n",
    "    FList = glob.glob(os.path.join(Folder, Pattern)) #Finding all Files in the given Folder \n",
    "    for Path in FList: \n",
    "        with open(Path, 'r') as file: \n",
    "            FileContents = file.read() #Reading File text\n",
    "            FileContents = FileContents.lower()\n",
    "            File_name = Path.strip(\"ResearchPapers\\\\.txt\")\n",
    "            FileContents = PunctuationRemove(FileContents)# Removing Punctuations\n",
    "            FileContents = FileContents.split() # Tokenizing string\n",
    "            Stemmer = PorterStemmer()\n",
    "            FileStem = []\n",
    "            #Applying Stemming to all the tokens\n",
    "            for words in list(FileContents):\n",
    "                FileStem.append(Stemmer.stem(words))\n",
    "            File_name = int(File_name)\n",
    "            Dictionary = DictionaryBuilder(FileStem,File_name)\n",
    "            Dictionary = sorted(Dictionary.items()) # Sorting the Dictionary by tokens\n",
    "            Dictionary = dict(Dictionary)\n",
    "    # Initializing all Document Vectors with 0 for every word\n",
    "    for i in range(1,31):\n",
    "         DocVectors[i] = [0] * len(Dictionary)\n",
    "    return Dictionary\n",
    "\n",
    "\n",
    "def PunctuationRemove(File):\n",
    "    File = File.replace('-', ' ') # Replacing hyphens with spaces\n",
    "    File = File.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    return(File)\n",
    "\n",
    "\n",
    "def DictionaryBuilder(File,File_Name):\n",
    "    Stop = open(r'Stopword-List.txt', 'r')\n",
    "    StopContents = Stop.read()\n",
    "    StopContents = StopContents.split()\n",
    "    for words in File: # Building Dictionary\n",
    "        if(words not in StopContents):\n",
    "            if(words not in Dictionary): # First time a word is added to Dictionary\n",
    "                Dictionary[words] = {}\n",
    "                Dictionary[words][File_Name] = 1 # Setting Term Frequency for the document to 1\n",
    "            else:\n",
    "                if(File_Name not in Dictionary[words]):\n",
    "                    Dictionary[words][File_Name] = 1 # Setting Term Frequency for the document to 1\n",
    "                else:\n",
    "                    Dictionary[words][File_Name] += 1 # Incrementing Term Frequency\n",
    "    return Dictionary   \n",
    "\n",
    "def BuildDocumentVectors():\n",
    "    for Index, Key in enumerate(Dictionary): # Traversing through words in Dictionary\n",
    "        for DocKeys in DocVectors.keys(): # Traversing through all Documents\n",
    "            if(DocKeys in Dictionary[Key]):\n",
    "                DocFreq = len(Dictionary[Key]) \n",
    "                InvertedDocFreq = round(math.log(len(DocVectors) / DocFreq, 10),2) # Calculating Inverted Document Frequency\n",
    "                TfIdf = InvertedDocFreq * Dictionary[Key][DocKeys]\n",
    "                DocVectors[DocKeys][Index] = TfIdf\n",
    "\n",
    "\n",
    "def QueryProcessor(Query):\n",
    "    Query = Query.split()\n",
    "    Query = QueryStemmer(Query)\n",
    "    QueryVector = [0] * len(Dictionary) # Initializing Query Vector\n",
    "    QueryDict = {}\n",
    "    for words in Query: # Building Dictionary for Query\n",
    "        if(words not in QueryDict): # First time a word is added to Dictionary\n",
    "            QueryDict[words] = 1\n",
    "        else:\n",
    "            QueryDict[words] += 1\n",
    "    for Index, Key in enumerate(Dictionary): # Traversing Dictionary\n",
    "            if(Key in QueryDict):\n",
    "                DocFreq = len(Dictionary[Key])\n",
    "                InvertedDocFreq = math.log(len(DocVectors) / DocFreq, 10)\n",
    "                TfIdf = InvertedDocFreq * QueryDict[Key]\n",
    "                QueryVector[Index] = TfIdf\n",
    "    return QueryVector\n",
    "\n",
    "\n",
    "def QueryStemmer(Query):\n",
    "    StemQuery = []\n",
    "    Stop = open(r'Stopword-List.txt', 'r')\n",
    "    StopContents = Stop.read()\n",
    "    StopContents = StopContents.split()\n",
    "    Stemmer = PorterStemmer()\n",
    "    Query = [Val for Val in Query if Val not in StopContents]\n",
    "    for words in Query:\n",
    "        StemQuery.append(Stemmer.stem(words))\n",
    "    return StemQuery  \n",
    "\n",
    "# Calculating Eucilidean Length for a Vector\n",
    "def EucDist(Vector):\n",
    "    Sum = 0\n",
    "    for i in Vector:\n",
    "        Sum += i ** 2\n",
    "    return(math.sqrt(Sum))\n",
    "\n",
    "def Solver(Query):\n",
    "    ResultList = []\n",
    "    QueryEucDist = EucDist(Query) # Calculating Euclidean Length for the Query\n",
    "    if QueryEucDist == 0: # Return empty list if the query vector is all zeros\n",
    "        return ResultList\n",
    "    for Doc in DocVectors.keys():\n",
    "        Cosine = 0\n",
    "        DotProduct = 0\n",
    "        DocEucDist = EucDist(DocVectors[Doc]) # Calculating Euclidean Length for a given Document\n",
    "        if DocEucDist == 0: # Skip calculation if the document vector is all zeros\n",
    "            continue\n",
    "        for i in range(0,len(Dictionary)):\n",
    "            if Query[i] == 0 or DocVectors[Doc][i] == 0: # Skip calculation if one of the TF-IDFs is zero\n",
    "                continue\n",
    "            else:\n",
    "                DotProduct += Query[i] * DocVectors[Doc][i]\n",
    "        Cosine = DotProduct / (QueryEucDist * DocEucDist)\n",
    "        if Cosine > 0.05: # Threshold\n",
    "            ResultList.append((Doc,Cosine))\n",
    "    ResultList = sorted(ResultList, key=lambda x:-x[1]) # Sort results according to Cosine value\n",
    "    return ResultList\n",
    "\n",
    "Dictionary = FileRead()\n",
    "BuildDocumentVectors()\n",
    "Query = ''\n",
    "while(1):\n",
    "    Query = input(\"Enter Query(Type -1 to exit): \")\n",
    "    if(Query == '-1'):\n",
    "        break\n",
    "    Query = QueryProcessor(str(Query))\n",
    "    print(Solver(Query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a87318b-5302-4cdf-8d10-8ce7159ec669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter Query (Type '-1' to exit):  MACHINE LEARNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(16, 0.130290461076697), (2, 0.09906766265502358), (24, 0.09594246904790407), (7, 0.09499223244572715), (1, 0.09090314851096121), (3, 0.08737820239762055), (17, 0.06266354326509087), (8, 0.061917936649195635)]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter Query (Type '-1' to exit):  TRANSFORMER\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(21, 0.38735031509185675), (18, 0.2048209890122228)]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter Query (Type '-1' to exit):  -1\n"
     ]
    }
   ],
   "source": [
    "################################################ ONLY REFACTORED\n",
    "import glob\n",
    "import nltk\n",
    "import os\n",
    "import string\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "import math \n",
    "\n",
    "Dictionary = {}  # Contains term frequencies for each word in each document\n",
    "DocVectors = {}  # Contains TF-IDF vectors for each document\n",
    "\n",
    "def FileRead(): \n",
    "    Folder = 'ResearchPapers'\n",
    "    Pattern = '*.txt' \n",
    "    FList = glob.glob(os.path.join(Folder, Pattern)) #Finding all Files in the given Folder \n",
    "    for Path in FList: \n",
    "        with open(Path, 'r') as file: \n",
    "            FileContents = file.read() #Reading File text\n",
    "            FileContents = FileContents.lower()\n",
    "            File_name = Path.strip(\"ResearchPapers\\\\.txt\")\n",
    "            FileContents = PunctuationRemove(FileContents)# Removing Punctuations\n",
    "            FileContents = FileContents.split() # Tokenizing string\n",
    "            Stemmer = PorterStemmer()\n",
    "            FileStem = []\n",
    "            #Applying Stemming to all the tokens\n",
    "            for words in list(FileContents):\n",
    "                FileStem.append(Stemmer.stem(words))\n",
    "            File_name = int(File_name)\n",
    "            Dictionary = DictionaryBuilder(FileStem,File_name)\n",
    "            Dictionary = sorted(Dictionary.items()) # Sorting the Dictionary by tokens\n",
    "            Dictionary = dict(Dictionary)\n",
    "    # Initializing all Document Vectors with 0 for every word\n",
    "    for i in range(1,31):\n",
    "         DocVectors[i] = [0] * len(Dictionary)\n",
    "    return Dictionary\n",
    "\n",
    "\n",
    "def PunctuationRemove(File):\n",
    "    # Function to remove punctuation marks from text\n",
    "    File = File.replace('-', ' ')  # Replacing hyphens with spaces\n",
    "    File = File.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    return File\n",
    "\n",
    "\n",
    "def DictionaryBuilder(File, File_Name):\n",
    "    # Function to build the dictionary with term frequencies\n",
    "    \n",
    "    # Open stopword list\n",
    "    Stop = open('Stopword-List.txt', 'r')\n",
    "    StopContents = Stop.read()\n",
    "    StopContents = StopContents.split()\n",
    "    \n",
    "    # Building dictionary\n",
    "    for word in File: \n",
    "        if word not in StopContents:\n",
    "            if word not in Dictionary:  # First time a word is added to dictionary\n",
    "                Dictionary[word] = {}\n",
    "                Dictionary[word][File_Name] = 1  # Setting term frequency for the document to 1\n",
    "            else:\n",
    "                if File_Name not in Dictionary[word]:\n",
    "                    Dictionary[word][File_Name] = 1  # Setting term frequency for the document to 1\n",
    "                else:\n",
    "                    Dictionary[word][File_Name] += 1  # Incrementing term frequency\n",
    "    \n",
    "    return Dictionary\n",
    "\n",
    "\n",
    "def BuildDocumentVectors():\n",
    "    # Function to build TF-IDF vectors for each document\n",
    "    \n",
    "    for Index, Key in enumerate(Dictionary): \n",
    "        for DocKeys in DocVectors.keys(): \n",
    "            if DocKeys in Dictionary[Key]:\n",
    "                DocFreq = len(Dictionary[Key]) \n",
    "                InvertedDocFreq = round(math.log(len(DocVectors) / DocFreq, 10), 2)  # Calculating IDF\n",
    "                TfIdf = InvertedDocFreq * Dictionary[Key][DocKeys]  # Calculating TF-IDF\n",
    "                DocVectors[DocKeys][Index] = TfIdf\n",
    "\n",
    "\n",
    "def QueryProcessor(Query):\n",
    "    # Function to process query text and convert it into a vector\n",
    "    \n",
    "    Query = Query.split()\n",
    "    Query = QueryStemmer(Query)  # Stemming query words\n",
    "    QueryVector = [0] * len(Dictionary)  # Initializing query vector\n",
    "    QueryDict = {}\n",
    "    \n",
    "    # Building dictionary for query\n",
    "    for word in Query: \n",
    "        if word not in QueryDict:\n",
    "            QueryDict[word] = 1\n",
    "        else:\n",
    "            QueryDict[word] += 1\n",
    "    \n",
    "    # Calculating TF-IDF for query vector\n",
    "    for Index, Key in enumerate(Dictionary): \n",
    "        if Key in QueryDict:\n",
    "            DocFreq = len(Dictionary[Key])\n",
    "            InvertedDocFreq = math.log(len(DocVectors) / DocFreq, 10)\n",
    "            TfIdf = InvertedDocFreq * QueryDict[Key]\n",
    "            QueryVector[Index] = TfIdf\n",
    "    \n",
    "    return QueryVector\n",
    "\n",
    "\n",
    "def QueryStemmer(Query):\n",
    "    # Function to stem words in query\n",
    "    \n",
    "    StemQuery = []\n",
    "    Stop = open('Stopword-List.txt', 'r')\n",
    "    StopContents = Stop.read()\n",
    "    StopContents = StopContents.split()\n",
    "    Stemmer = PorterStemmer()\n",
    "    Query = [Val for Val in Query if Val not in StopContents]\n",
    "    \n",
    "    # Stemming query words\n",
    "    for word in Query:\n",
    "        StemQuery.append(Stemmer.stem(word))\n",
    "    \n",
    "    return StemQuery  \n",
    "\n",
    "\n",
    "def EucDist(Vector):\n",
    "    # Function to calculate Euclidean distance for a vector\n",
    "    \n",
    "    Sum = 0\n",
    "    for i in Vector:\n",
    "        Sum += i ** 2\n",
    "    return math.sqrt(Sum)\n",
    "\n",
    "\n",
    "def Solver(Query):\n",
    "    # Function to solve the query and retrieve relevant documents\n",
    "    \n",
    "    ResultList = []\n",
    "    QueryEucDist = EucDist(Query)  # Calculating Euclidean length for the query\n",
    "    \n",
    "    if QueryEucDist == 0: \n",
    "        return ResultList  # Return empty list if the query vector is all zeros\n",
    "    \n",
    "    for Doc in DocVectors.keys():\n",
    "        Cosine = 0\n",
    "        DotProduct = 0\n",
    "        DocEucDist = EucDist(DocVectors[Doc])  # Calculating Euclidean length for a given document\n",
    "        \n",
    "        if DocEucDist == 0: \n",
    "            continue  # Skip calculation if the document vector is all zeros\n",
    "        \n",
    "        for i in range(0, len(Dictionary)):\n",
    "            if Query[i] == 0 or DocVectors[Doc][i] == 0: \n",
    "                continue  # Skip calculation if one of the TF-IDFs is zero\n",
    "            else:\n",
    "                DotProduct += Query[i] * DocVectors[Doc][i]\n",
    "        \n",
    "        Cosine = DotProduct / (QueryEucDist * DocEucDist)\n",
    "        \n",
    "        if Cosine > 0.05:  # Threshold\n",
    "            ResultList.append((Doc, Cosine))\n",
    "    \n",
    "    # Sort results according to cosine value\n",
    "    ResultList = sorted(ResultList, key=lambda x:-x[1])  \n",
    "    return ResultList\n",
    "\n",
    "# Reading files, building dictionaries, and constructing document vectors\n",
    "Dictionary = FileRead()\n",
    "BuildDocumentVectors()\n",
    "\n",
    "# Query processing and solving loop\n",
    "while True:\n",
    "    Query = input(\"Enter Query (Type '-1' to exit): \")\n",
    "    if Query == '-1':\n",
    "        break\n",
    "    Query = QueryProcessor(str(Query))\n",
    "    print(Solver(Query))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
