{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "08f55c58-6705-4020-bb65-53ca4b136f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Explainable Artificial Intelligence': {'precision': 0.6666666666666666, 'recall': 1.0, 'f1-score': 0.8, 'support': 2}, 'Feature Selection': {'precision': 1.0, 'recall': 0.5, 'f1-score': 0.6666666666666666, 'support': 2}, 'accuracy': 0.75, 'macro avg': {'precision': 0.8333333333333333, 'recall': 0.75, 'f1-score': 0.7333333333333334, 'support': 4}, 'weighted avg': {'precision': 0.8333333333333333, 'recall': 0.75, 'f1-score': 0.7333333333333334, 'support': 4}}\n",
      "Precision: 0.8333333333333333\n",
      "Recall: 0.75\n",
      "F1 Score: 0.7333333333333334\n",
      "Accuracy: 0.75\n",
      "\n",
      "\n",
      "//////////////////////////////////////////////////////////\n",
      "['Explainable Artificial Intelligence']\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import string\n",
    "from nltk.stem import PorterStemmer\n",
    "import math \n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "Dictionary = {} # Global dictionary\n",
    "DocVectors = {} # Global dictionary for Document Vectors\n",
    "\n",
    "def load_data():\n",
    "    flag = False\n",
    "    if os.path.exists('Dict.json'):\n",
    "        flag = True\n",
    "        print(\"reading Dict\")\n",
    "        with open('Dict.json', 'r') as f:\n",
    "            Dictionary.update(json.load(f))\n",
    "    if os.path.exists('TFIDFVec.json'):\n",
    "        flag = True\n",
    "        print(\"reading TFIDFVec\")\n",
    "        with open('TFIDFVec.json', 'r') as f:\n",
    "            DocVectors.update(json.load(f))\n",
    "    return flag\n",
    "\n",
    "def FileRead(): \n",
    "    Folder = 'ResearchPapers'\n",
    "    Pattern = '*.txt' \n",
    "    FList = glob.glob(os.path.join(Folder, Pattern)) #Finding all Files in the given Folder \n",
    "    for Path in FList: \n",
    "        with open(Path, 'r') as file: \n",
    "            FileContents = file.read() #Reading File text\n",
    "            FileContents = FileContents.lower()\n",
    "            File_name = Path.strip(\"ResearchPapers\\\\.txt\")\n",
    "            FileContents = PunctuationRemove(FileContents)# Removing Punctuations\n",
    "            FileContents = FileContents.split() # Tokenizing string\n",
    "            Stemmer = PorterStemmer()\n",
    "            FileStem = []\n",
    "            #Applying Stemming to all the tokens\n",
    "            for words in list(FileContents):\n",
    "                FileStem.append(Stemmer.stem(words))\n",
    "            File_name = int(File_name)\n",
    "            Dictionary = DictionaryBuilder(FileStem,File_name)\n",
    "            Dictionary = sorted(Dictionary.items()) # Sorting the Dictionary by tokens\n",
    "            Dictionary = dict(Dictionary)\n",
    "    with open('Dict.json', 'w') as f:\n",
    "        json.dump(Dictionary, f)\n",
    "    # Initializing all Document Vectors with 0 for every word\n",
    "    for i in range(1,27):\n",
    "        if (i == 4 or i==5 or i==6 or i==10 or i==19 or i==20):\n",
    "            continue\n",
    "        DocVectors[i] = [0] * len(Dictionary)\n",
    "    return Dictionary\n",
    "\n",
    "def PunctuationRemove(File):\n",
    "    # Function to remove punctuation marks from text\n",
    "    File = File.replace('-', ' ')  # Replacing hyphens with spaces\n",
    "    File = File.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    return File\n",
    "\n",
    "def DictionaryBuilder(File,File_Name):\n",
    "    Stop = open(r'Stopword-List.txt', 'r')\n",
    "    StopContents = Stop.read()\n",
    "    StopContents = StopContents.split()\n",
    "    for words in File: # Building Dictionary\n",
    "        if(words not in StopContents):\n",
    "            if(words not in Dictionary): # First time a word is added to Dictionary\n",
    "                Dictionary[words] = {}\n",
    "                Dictionary[words][File_Name] = 1 # Setting Term Frequency for the document to 1\n",
    "            else:\n",
    "                if(File_Name not in Dictionary[words]):\n",
    "                    Dictionary[words][File_Name] = 1 # Setting Term Frequency for the document to 1\n",
    "                else:\n",
    "                    Dictionary[words][File_Name] += 1 # Incrementing Term Frequency\n",
    "    return Dictionary   \n",
    "\n",
    "def BuildDocumentVectors():\n",
    "    for Index, Key in enumerate(Dictionary): # Traversing through words in Dictionary\n",
    "        for DocKeys in DocVectors.keys(): # Traversing through all Documents\n",
    "            if(DocKeys in Dictionary[Key]):\n",
    "                DocFreq = len(Dictionary[Key]) \n",
    "                InvertedDocFreq = round(math.log(len(DocVectors) / DocFreq, 10),2) # Calculating Inverted Document Frequency\n",
    "                TfIdf = InvertedDocFreq * Dictionary[Key][DocKeys] \n",
    "                DocVectors[DocKeys][Index] = TfIdf\n",
    "    with open('TFIDFVec.json', 'w') as f:\n",
    "        json.dump(DocVectors, f)\n",
    "\n",
    "def QueryProcessor(Query):\n",
    "    Query = Query.split()\n",
    "    Query = QueryStemmer(Query)\n",
    "    QueryVector = [0] * len(Dictionary) # Initializing Query Vector \n",
    "    QueryDict = {}\n",
    "    for words in Query: # Building Dictionary for Query\n",
    "        if(words not in QueryDict): # First time a word is added to Dictionary\n",
    "            QueryDict[words] = 1\n",
    "        else:\n",
    "            QueryDict[words] += 1\n",
    "    for Index, Key in enumerate(Dictionary): # Traversing Dictionary\n",
    "            if(Key in QueryDict):\n",
    "                DocFreq = len(Dictionary[Key])\n",
    "                InvertedDocFreq = math.log(len(DocVectors) / DocFreq, 10)\n",
    "                TfIdf = InvertedDocFreq * QueryDict[Key]\n",
    "                QueryVector[Index] = TfIdf\n",
    "    return QueryVector\n",
    "\n",
    "def QueryStemmer(Query):\n",
    "    StemQuery = []\n",
    "    Stop = open(r'Stopword-List.txt', 'r')\n",
    "    StopContents = Stop.read()\n",
    "    StopContents = StopContents.split()\n",
    "    Stemmer = PorterStemmer()\n",
    "    Query = [Val for Val in Query if Val not in StopContents]\n",
    "    for words in Query:\n",
    "        StemQuery.append(Stemmer.stem(words))\n",
    "    return StemQuery  \n",
    "\n",
    "Dictionary = FileRead()\n",
    "BuildDocumentVectors()\n",
    "\n",
    "# def search_query():\n",
    "#     query = input(\"Enter your query: \")\n",
    "#     query_vector = QueryProcessor(query)\n",
    "#     results = Solver(query_vector)\n",
    "\n",
    "#     if not results:\n",
    "#         print(\"No documents match the query.\")\n",
    "#     else:\n",
    "#         print(\"Search Results sorted by cosine similarity:\")\n",
    "#         for result in results:\n",
    "#             print(f\"Document ID: {result[0]}, Cosine Similarity: {result[1]}\")\n",
    "\n",
    "# # Call the search_query function to start searching\n",
    "# search_query()\n",
    "\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "doc_classes = {\n",
    "    '1': \"Explainable Artificial Intelligence\",\n",
    "    '2': \"Explainable Artificial Intelligence\",\n",
    "    '3': \"Explainable Artificial Intelligence\",\n",
    "    '7': \"Explainable Artificial Intelligence\",\n",
    "    '8': \"Heart Failure\",\n",
    "    '9': \"Heart Failure\",\n",
    "    '11': \"Heart Failure\",\n",
    "    '12': \"Time Series Forecasting\",\n",
    "    '13': \"Time Series Forecasting\",\n",
    "    '14': \"Time Series Forecasting\",\n",
    "    '15': \"Time Series Forecasting\",\n",
    "    '16': \"Time Series Forecasting\",\n",
    "    '17': \"Transformer Model\",\n",
    "    '18': \"Transformer Model\",\n",
    "    '21': \"Transformer Model\",\n",
    "    '22': \"Feature Selection\",\n",
    "    '23': \"Feature Selection\",\n",
    "    '24': \"Feature Selection\",\n",
    "    '25': \"Feature Selection\",\n",
    "    '26': \"Feature Selection\"\n",
    "}\n",
    "\n",
    "def LoadTFIDFVectors(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        tfidf_vectors = json.load(f)\n",
    "    return tfidf_vectors\n",
    "\n",
    "def PrepareData(tfidf_vectors):\n",
    "    X = []  # Feature vectors\n",
    "    y = []  # Class labels\n",
    "    \n",
    "    for doc_id, vector in tfidf_vectors.items():\n",
    "        X.append(vector)\n",
    "        # Assuming you have a dictionary 'doc_classes' mapping document IDs to class labels\n",
    "        y.append(doc_classes[doc_id])\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Load TF-IDF vectors from JSON file\n",
    "tfidf_vectors = LoadTFIDFVectors('TFIDFVec.json')\n",
    "\n",
    "# Prepare data for classification\n",
    "X, y = PrepareData(tfidf_vectors)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the k-NN classifier\n",
    "# k = int(input(\"Enter K\"))  # Number of neighbors\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=3)\n",
    "knn_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Classify the test data\n",
    "y_pred = knn_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "print(report)\n",
    "precision = report['weighted avg']['precision']\n",
    "recall = report['weighted avg']['recall']\n",
    "f1_score = report['weighted avg']['f1-score']\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1_score)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "\n",
    "paragraph6 = \"\"\"\n",
    "Transformer are translating text and speech in near real-time, opening meetings and classrooms to diverse and hearing-impaired attendees. They’re helping researchers understand the chains of genes in DNA and amino acids in proteins in ways that can speed drug design.\n",
    "\"\"\"\n",
    "print(\"\\n\\n//////////////////////////////////////////////////////////\")\n",
    "Vector_for_query=QueryProcessor(paragraph6)\n",
    "Vector_for_query = np.array(Vector_for_query).reshape(1, -1)\n",
    "Answer=knn_classifier.predict(Vector_for_query)\n",
    "print(Answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "20f6a204-db2a-488d-bd20-7abbf4c72e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Dict.json\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the value of K for KNN:  5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "{'Explainable Artificial Intelligence': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 2}, 'Feature Selection': {'precision': 0.5, 'recall': 1.0, 'f1-score': 0.6666666666666666, 'support': 2}, 'accuracy': 0.5, 'macro avg': {'precision': 0.25, 'recall': 0.5, 'f1-score': 0.3333333333333333, 'support': 4}, 'weighted avg': {'precision': 0.25, 'recall': 0.5, 'f1-score': 0.3333333333333333, 'support': 4}}\n",
      "Accuracy: 0.5\n",
      "Precision: 0.25\n",
      "Recall: 0.5\n",
      "F1 Score: 0.3333333333333333\n",
      "Predicted class for the query paragraph: ['Feature Selection']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#######################################   CLASSIFICATION USING KNN CLASSIFICATION ALGORITHM\n",
    "import glob\n",
    "import os\n",
    "import string\n",
    "from nltk.stem import PorterStemmer\n",
    "import math \n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Global dictionaries for storing TF-IDF vectors and document classes\n",
    "Dictionary = {}\n",
    "DocClasses = {\n",
    "    '1': \"Explainable Artificial Intelligence\",\n",
    "    '2': \"Explainable Artificial Intelligence\",\n",
    "    '3': \"Explainable Artificial Intelligence\",\n",
    "    '7': \"Explainable Artificial Intelligence\",\n",
    "    '8': \"Heart Failure\",\n",
    "    '9': \"Heart Failure\",\n",
    "    '11': \"Heart Failure\",\n",
    "    '12': \"Time Series Forecasting\",\n",
    "    '13': \"Time Series Forecasting\",\n",
    "    '14': \"Time Series Forecasting\",\n",
    "    '15': \"Time Series Forecasting\",\n",
    "    '16': \"Time Series Forecasting\",\n",
    "    '17': \"Transformer Model\",\n",
    "    '18': \"Transformer Model\",\n",
    "    '21': \"Transformer Model\",\n",
    "    '22': \"Feature Selection\",\n",
    "    '23': \"Feature Selection\",\n",
    "    '24': \"Feature Selection\",\n",
    "    '25': \"Feature Selection\",\n",
    "    '26': \"Feature Selection\"\n",
    "}\n",
    "\n",
    "def load_data():\n",
    "    flag = False\n",
    "    if os.path.exists('Dict.json'):\n",
    "        flag = True\n",
    "        print(\"Reading Dict.json\")\n",
    "        with open('Dict.json', 'r') as f:\n",
    "            Dictionary.update(json.load(f))\n",
    "    return flag\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    return text\n",
    "\n",
    "def tokenize_and_stem(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = text.split()\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    return stemmed_tokens\n",
    "\n",
    "def build_dictionary():\n",
    "    Folder = 'ResearchPapers'\n",
    "    Pattern = '*.txt' \n",
    "    FList = glob.glob(os.path.join(Folder, Pattern)) \n",
    "    for Path in FList: \n",
    "        with open(Path, 'r') as file: \n",
    "            FileContents = file.read() \n",
    "            FileContents = preprocess_text(FileContents)\n",
    "            FileContents = tokenize_and_stem(FileContents)\n",
    "            File_name = int(os.path.basename(Path).split('.')[0])\n",
    "            Dictionary = dictionary_builder(FileContents, File_name)\n",
    "    with open('Dict.json', 'w') as f:\n",
    "        json.dump(Dictionary, f)\n",
    "\n",
    "def dictionary_builder(file, file_name):\n",
    "    Stop = open(r'Stopword-List.txt', 'r')\n",
    "    StopContents = Stop.read()\n",
    "    StopContents = StopContents.split()\n",
    "    for word in file: \n",
    "        if word not in StopContents:\n",
    "            if word not in Dictionary:\n",
    "                Dictionary[word] = {}\n",
    "                Dictionary[word][file_name] = 1\n",
    "            else:\n",
    "                if file_name not in Dictionary[word]:\n",
    "                    Dictionary[word][file_name] = 1\n",
    "                else:\n",
    "                    Dictionary[word][file_name] += 1\n",
    "    return Dictionary   \n",
    "\n",
    "def build_document_vectors():\n",
    "    global DocVectors\n",
    "    DocVectors = {doc_key: [0] * len(Dictionary) for doc_key in range(1, 27)}\n",
    "    for index, key in enumerate(Dictionary):\n",
    "        for doc_key in DocVectors.keys():\n",
    "            if doc_key in Dictionary[key]:\n",
    "                doc_freq = len(Dictionary[key])\n",
    "                inverted_doc_freq = round(math.log(len(DocVectors) / doc_freq, 10), 2)\n",
    "                tf_idf = inverted_doc_freq * Dictionary[key][doc_key] \n",
    "                DocVectors[doc_key][index] = tf_idf\n",
    "    with open('TFIDFVec.json', 'w') as f:\n",
    "        json.dump(DocVectors, f)\n",
    "\n",
    "def prepare_data():\n",
    "    X = []\n",
    "    y = []\n",
    "    for doc_id, vector in DocVectors.items():\n",
    "        X.append(vector)\n",
    "        y.append(DocClasses[str(doc_id)])\n",
    "    return X, y\n",
    "\n",
    "def train_knn_classifier(X_train, y_train, k):\n",
    "    knn_classifier = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn_classifier.fit(X_train, y_train)\n",
    "    return knn_classifier\n",
    "\n",
    "\n",
    "def evaluate_classifier(classifier, X_test, y_test):\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = report['weighted avg']['precision']\n",
    "    recall = report['weighted avg']['recall']\n",
    "    f1_score = report['weighted avg']['f1-score']\n",
    "    return report, accuracy, precision, recall, f1_score\n",
    "\n",
    "def main():\n",
    "    if not load_data():\n",
    "        build_dictionary()\n",
    "        build_document_vectors()\n",
    "    X, y = prepare_data()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    k = int(input(\"Enter the value of K for KNN: \"))\n",
    "    knn_classifier = train_knn_classifier(X_train, y_train, k)\n",
    "    report, accuracy, precision, recall, f1_score = evaluate_classifier(knn_classifier, X_test, y_test)\n",
    "    print(\"Classification Report:\")\n",
    "    print(report)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1 Score:\", f1_score)\n",
    "\n",
    "    # Classify a new query\n",
    "    paragraph = \"\"\"\n",
    "    Transformers are translating text and speech in near real-time, opening meetings and classrooms to diverse and hearing-impaired attendees. They’re helping researchers understand the chains of genes in DNA and amino acids in proteins in ways that can speed drug design.\n",
    "    \"\"\"\n",
    "    query_vector = np.array(QueryProcessor(paragraph)).reshape(1, -1)\n",
    "    answer = knn_classifier.predict(query_vector)\n",
    "    print(\"Predicted class for the query paragraph:\", answer)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e3689ee1-e0dd-4982-9b60-0830ab6aaaf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the number of clusters (K):  5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering Results:\n",
      "==================================================\n",
      "Purity: 0.4\n",
      "Silhouette Score: 0.20873610487625385\n",
      "Rand Index: 0.3894736842105263\n",
      "==================================================\n",
      "Cluster 2:\n",
      "1\n",
      "2\n",
      "3\n",
      "8\n",
      "9\n",
      "11\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "21\n",
      "23\n",
      "24\n",
      "25\n",
      "==================================================\n",
      "Cluster 1:\n",
      "7\n",
      "==================================================\n",
      "Cluster 3:\n",
      "12\n",
      "==================================================\n",
      "Cluster 5:\n",
      "22\n",
      "==================================================\n",
      "Cluster 4:\n",
      "26\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1382: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#######################################   CLUSTERING USING K-MEANS CLUSTERING ALGORITHM\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics.cluster import rand_score\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load TF-IDF vectors from JSON file\n",
    "def load_tfidf_vectors(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        tfidf_vectors = json.load(f)\n",
    "    return tfidf_vectors\n",
    "\n",
    "# Prepare TF-IDF vectors for clustering\n",
    "def prepare_data(tfidf_vectors):\n",
    "    X = np.array(list(tfidf_vectors.values()))\n",
    "    doc_ids = list(tfidf_vectors.keys())\n",
    "    return X, doc_ids\n",
    "\n",
    "# Perform k-Means clustering\n",
    "def perform_clustering(X, k):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(X)\n",
    "    return cluster_labels, kmeans\n",
    "\n",
    "# Evaluate clustering using Purity\n",
    "def calculate_purity(cluster_labels, doc_ids, doc_classes):\n",
    "    cluster_docs = defaultdict(list)\n",
    "    for i, label in enumerate(cluster_labels):\n",
    "        cluster_docs[label].append(doc_ids[i])\n",
    "    \n",
    "    purity_sum = 0\n",
    "    for docs in cluster_docs.values():\n",
    "        true_labels = [doc_classes[doc_id] for doc_id in docs]\n",
    "        majority_class = max(set(true_labels), key=true_labels.count)\n",
    "        purity_sum += true_labels.count(majority_class)\n",
    "    \n",
    "    purity = purity_sum / len(doc_ids)\n",
    "    return purity\n",
    "\n",
    "# Display documents in each cluster\n",
    "def display_clusters(cluster_labels, doc_ids):\n",
    "    cluster_docs = defaultdict(list)\n",
    "    for i, label in enumerate(cluster_labels):\n",
    "        cluster_docs[label].append(doc_ids[i])\n",
    "    \n",
    "    for cluster, docs in cluster_docs.items():\n",
    "        print(f\"Cluster {cluster + 1}:\")\n",
    "        for doc_id in docs:\n",
    "            print(doc_id)\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "# Load document classes\n",
    "doc_classes = {\n",
    "    '1': \"Explainable Artificial Intelligence\",\n",
    "    '2': \"Explainable Artificial Intelligence\",\n",
    "    '3': \"Explainable Artificial Intelligence\",\n",
    "    '7': \"Explainable Artificial Intelligence\",\n",
    "    '8': \"Heart Failure\",\n",
    "    '9': \"Heart Failure\",\n",
    "    '11': \"Heart Failure\",\n",
    "    '12': \"Time Series Forecasting\",\n",
    "    '13': \"Time Series Forecasting\",\n",
    "    '14': \"Time Series Forecasting\",\n",
    "    '15': \"Time Series Forecasting\",\n",
    "    '16': \"Time Series Forecasting\",\n",
    "    '17': \"Transformer Model\",\n",
    "    '18': \"Transformer Model\",\n",
    "    '21': \"Transformer Model\",\n",
    "    '22': \"Feature Selection\",\n",
    "    '23': \"Feature Selection\",\n",
    "    '24': \"Feature Selection\",\n",
    "    '25': \"Feature Selection\",\n",
    "    '26': \"Feature Selection\"\n",
    "}\n",
    "\n",
    "# Load TF-IDF vectors\n",
    "tfidf_vectors = load_tfidf_vectors('TFIDFVec.json')\n",
    "\n",
    "# Prepare data for clustering\n",
    "X, doc_ids = prepare_data(tfidf_vectors)\n",
    "\n",
    "# Choose K\n",
    "k = int(input(\"Enter the number of clusters (K): \"))\n",
    "\n",
    "# Perform clustering\n",
    "cluster_labels, kmeans = perform_clustering(X, k)\n",
    "\n",
    "# Evaluate clustering using Purity\n",
    "purity = calculate_purity(cluster_labels, doc_ids, doc_classes)\n",
    "\n",
    "# Evaluate clustering using Silhouette Score\n",
    "silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "\n",
    "# Evaluate clustering using Rand Index\n",
    "rand_idx = rand_score([doc_classes[doc_id] for doc_id in doc_ids], cluster_labels)\n",
    "\n",
    "# Display clustering results\n",
    "print(\"Clustering Results:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Purity:\", purity)\n",
    "print(\"Silhouette Score:\", silhouette_avg)\n",
    "print(\"Rand Index:\", rand_idx)\n",
    "print(\"=\" * 50)\n",
    "display_clusters(cluster_labels, doc_ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
