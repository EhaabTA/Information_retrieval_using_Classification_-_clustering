{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81998ac0-d9e0-4acf-afa9-42d1e43013c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your desired threshold :  0.05\n",
      "Enter Query (Type '-1' to exit):  machine learning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(16, 0.130290461076697), (2, 0.09906766265502358), (24, 0.09594246904790407), (7, 0.09499223244572715), (1, 0.09090314851096121), (3, 0.08737820239762055), (17, 0.06266354326509087), (8, 0.061917936649195635)]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter Query (Type '-1' to exit):  -1\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import scrolledtext\n",
    "import math\n",
    "import string\n",
    "import os\n",
    "import glob\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "Dictionary = {}  # Contains term frequencies for each word in each document\n",
    "DocVectors = {}  # Contains TF-IDF vectors for each document\n",
    "\n",
    "def FileRead():\n",
    "    # Function to read files, preprocess text, and build the dictionary\n",
    "    \n",
    "    Folder = 'ResearchPapers'\n",
    "    Pattern = '*.txt' \n",
    "    FList = glob.glob(os.path.join(Folder, Pattern)) #Finding all Files in the given Folder \n",
    "    \n",
    "    for Path in FList: \n",
    "        with open(Path, 'r') as file: \n",
    "            FileContents = file.read() #Reading File text\n",
    "            FileContents = FileContents.lower()\n",
    "            File_name = Path.strip(\"ResearchPapers\\\\.txt\")\n",
    "            FileContents = PunctuationRemove(FileContents)# Removing Punctuations\n",
    "            FileContents = FileContents.split() # Tokenizing string\n",
    "            Stemmer = PorterStemmer()\n",
    "            FileStem = []\n",
    "            #Applying Stemming to all the tokens\n",
    "            for words in list(FileContents):\n",
    "                FileStem.append(Stemmer.stem(words))\n",
    "            File_name = int(File_name)\n",
    "            Dictionary = DictionaryBuilder(FileStem,File_name)\n",
    "            Dictionary = sorted(Dictionary.items()) # Sorting the Dictionary by tokens\n",
    "            Dictionary = dict(Dictionary)\n",
    "    \n",
    "    # Initializing all Document Vectors with 0 for every word\n",
    "    for i in range(1,26):\n",
    "         DocVectors[i] = [0] * len(Dictionary)\n",
    "    \n",
    "    return Dictionary\n",
    "\n",
    "def PunctuationRemove(File):\n",
    "    # Function to remove punctuation marks from text\n",
    "    File = File.replace('-', ' ')  # Replacing hyphens with spaces\n",
    "    File = File.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    return File\n",
    "\n",
    "def DictionaryBuilder(File, File_Name):\n",
    "    # Function to build the dictionary with term frequencies\n",
    "    \n",
    "    Stop = open('Stopword-List.txt', 'r')\n",
    "    StopContents = Stop.read()\n",
    "    StopContents = StopContents.split()\n",
    "    \n",
    "    # Building dictionary\n",
    "    for word in File: \n",
    "        if word not in StopContents:\n",
    "            if word not in Dictionary:  # First time a word is added to dictionary\n",
    "                Dictionary[word] = {}\n",
    "                Dictionary[word][File_Name] = 1  # Setting term frequency for the document to 1\n",
    "            else:\n",
    "                if File_Name not in Dictionary[word]:\n",
    "                    Dictionary[word][File_Name] = 1  # Setting term frequency for the document to 1\n",
    "                else:\n",
    "                    Dictionary[word][File_Name] += 1  # Incrementing term frequency\n",
    "    \n",
    "    return Dictionary\n",
    "\n",
    "def BuildDocumentVectors():\n",
    "    # Function to build TF-IDF vectors for each document\n",
    "    \n",
    "    for Index, Key in enumerate(Dictionary): \n",
    "        for DocKeys in DocVectors.keys(): \n",
    "            if DocKeys in Dictionary[Key]:\n",
    "                DocFreq = len(Dictionary[Key]) \n",
    "                InvertedDocFreq = round(math.log(len(DocVectors) / DocFreq, 10), 2)  # Calculating IDF\n",
    "                TfIdf = InvertedDocFreq * Dictionary[Key][DocKeys]  # Calculating TF-IDF\n",
    "                DocVectors[DocKeys][Index] = TfIdf\n",
    "\n",
    "def QueryProcessor(Query):\n",
    "    # Function to process query text and convert it into a vector\n",
    "    \n",
    "    Query = Query.split()\n",
    "    Query = QueryStemmer(Query)  # Stemming query words\n",
    "    QueryVector = [0] * len(Dictionary)  # Initializing query vector\n",
    "    QueryDict = {}\n",
    "    \n",
    "    # Building dictionary for query\n",
    "    for word in Query: \n",
    "        if word not in QueryDict:\n",
    "            QueryDict[word] = 1\n",
    "        else:\n",
    "            QueryDict[word] += 1\n",
    "    \n",
    "    # Calculating TF-IDF for query vector\n",
    "    for Index, Key in enumerate(Dictionary): \n",
    "        if Key in QueryDict:\n",
    "            DocFreq = len(Dictionary[Key])\n",
    "            InvertedDocFreq = math.log(len(DocVectors) / DocFreq, 10)\n",
    "            TfIdf = InvertedDocFreq * QueryDict[Key]\n",
    "            QueryVector[Index] = TfIdf\n",
    "    \n",
    "    return QueryVector\n",
    "\n",
    "def QueryStemmer(Query):\n",
    "    # Function to stem words in query\n",
    "    \n",
    "    StemQuery = []\n",
    "    Stop = open('Stopword-List.txt', 'r')\n",
    "    StopContents = Stop.read()\n",
    "    StopContents = StopContents.split()\n",
    "    Stemmer = PorterStemmer()\n",
    "    Query = [Val for Val in Query if Val not in StopContents]\n",
    "    \n",
    "    # Stemming query words\n",
    "    for word in Query:\n",
    "        StemQuery.append(Stemmer.stem(word))\n",
    "    \n",
    "    return StemQuery  \n",
    "\n",
    "def EucDist(Vector):\n",
    "    # Function to calculate Euclidean distance for a vector\n",
    "    \n",
    "    Sum = 0\n",
    "    for i in Vector:\n",
    "        Sum += i ** 2\n",
    "    return math.sqrt(Sum)\n",
    "\n",
    "def Solver(Query):\n",
    "    # Function to solve the query and retrieve relevant documents\n",
    "    \n",
    "    ResultList = []\n",
    "    QueryEucDist = EucDist(Query)  # Calculating Euclidean length for the query\n",
    "    \n",
    "    if QueryEucDist == 0: \n",
    "        return ResultList  # Return empty list if the query vector is all zeros\n",
    "    \n",
    "    for Doc in DocVectors.keys():\n",
    "        Cosine = 0\n",
    "        DotProduct = 0\n",
    "        DocEucDist = EucDist(DocVectors[Doc])  # Calculating Euclidean length for a given document\n",
    "        \n",
    "        if DocEucDist == 0: \n",
    "            continue  # Skip calculation if the document vector is all zeros\n",
    "        \n",
    "        for i in range(0, len(Dictionary)):\n",
    "            if Query[i] == 0 or DocVectors[Doc][i] == 0: \n",
    "                continue  # Skip calculation if one of the TF-IDFs is zero\n",
    "            else:\n",
    "                DotProduct += Query[i] * DocVectors[Doc][i]\n",
    "        \n",
    "        Cosine = DotProduct / (QueryEucDist * DocEucDist)\n",
    "        \n",
    "        if Cosine > 0.05:  # Threshold\n",
    "            ResultList.append((Doc, Cosine))\n",
    "    \n",
    "    # Sort results according to cosine value\n",
    "    ResultList = sorted(ResultList, key=lambda x:-x[1])  \n",
    "    return ResultList\n",
    "\n",
    "def on_click():\n",
    "    # Function to handle query button click event\n",
    "    \n",
    "    query_text = query_entry.get(\"1.0\", tk.END).strip()\n",
    "    if query_text:\n",
    "        Query = QueryProcessor(query_text)\n",
    "        results = Solver(Query)\n",
    "        result_text.delete('1.0', tk.END)\n",
    "        for result in results:\n",
    "            result_text.insert(tk.END, f\"Document: {result[0]}, Cosine Similarity: {result[1]}\\n\")\n",
    "\n",
    "# Reading files, building dictionaries, and constructing document vectors\n",
    "Dictionary = FileRead()\n",
    "BuildDocumentVectors()\n",
    "\n",
    "# Creating GUI using tkinter\n",
    "root = tk.Tk()\n",
    "root.title(\"Document Search Engine\")\n",
    "\n",
    "root.configure(bg=\"#000000\")\n",
    "\n",
    "query_frame = tk.Frame(root)\n",
    "query_frame.pack(pady=10)\n",
    "\n",
    "query_label = tk.Label(query_frame, text=\"Enter your Query:\", bg=\"#000000\", fg=\"#FFFFFF\")  # Set text and background color\n",
    "query_label.pack(side=tk.LEFT)\n",
    "\n",
    "query_entry = tk.Text(query_frame, height=5, width=100, bg=\"#FFFFFF\", fg=\"#000000\")  # Set text and background color\n",
    "query_entry.pack(side=tk.LEFT)\n",
    "\n",
    "query_button = tk.Button(query_frame, text=\"Search query\", command=on_click, bg=\"#FFFFFF\", fg=\"#000000\")  # Set button color\n",
    "query_button.pack(side=tk.LEFT)\n",
    "\n",
    "result_text = scrolledtext.ScrolledText(root, width=80, height=20, wrap=tk.WORD, bg=\"#000000\", fg=\"#FFFFFF\")  # Set text and background color\n",
    "result_text.pack(padx=10, pady=10)\n",
    "\n",
    "root.mainloop()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7a57a4-97ec-4877-a1c5-69d90ce78698",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################ ONLY REFACTORED\n",
    "import glob\n",
    "import nltk\n",
    "import os\n",
    "import string\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "import math \n",
    "\n",
    "Dictionary = {}  # Contains term frequencies for each word in each document\n",
    "DocVectors = {}  # Contains TF-IDF vectors for each document\n",
    "\n",
    "def FileRead(): \n",
    "    Folder = 'ResearchPapers'\n",
    "    Pattern = '*.txt' \n",
    "    FList = glob.glob(os.path.join(Folder, Pattern)) #Finding all Files in the given Folder \n",
    "    for Path in FList: \n",
    "        with open(Path, 'r') as file: \n",
    "            FileContents = file.read() #Reading File text\n",
    "            FileContents = FileContents.lower()\n",
    "            File_name = Path.strip(\"ResearchPapers\\\\.txt\")\n",
    "            FileContents = PunctuationRemove(FileContents)# Removing Punctuations\n",
    "            FileContents = FileContents.split() # Tokenizing string\n",
    "            Stemmer = PorterStemmer()\n",
    "            FileStem = []\n",
    "            #Applying Stemming to all the tokens\n",
    "            for words in list(FileContents):\n",
    "                FileStem.append(Stemmer.stem(words))\n",
    "            File_name = int(File_name)\n",
    "            Dictionary = DictionaryBuilder(FileStem,File_name)\n",
    "            Dictionary = sorted(Dictionary.items()) # Sorting the Dictionary by tokens\n",
    "            Dictionary = dict(Dictionary)\n",
    "    # Initializing all Document Vectors with 0 for every word\n",
    "    for i in range(1,31):\n",
    "         DocVectors[i] = [0] * len(Dictionary)\n",
    "    return Dictionary\n",
    "\n",
    "\n",
    "def PunctuationRemove(File):\n",
    "    # Function to remove punctuation marks from text\n",
    "    File = File.replace('-', ' ')  # Replacing hyphens with spaces\n",
    "    File = File.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    return File\n",
    "\n",
    "\n",
    "def DictionaryBuilder(File, File_Name):\n",
    "    # Function to build the dictionary with term frequencies\n",
    "    \n",
    "    # Open stopword list\n",
    "    Stop = open('Stopword-List.txt', 'r')\n",
    "    StopContents = Stop.read()\n",
    "    StopContents = StopContents.split()\n",
    "    \n",
    "    # Building dictionary\n",
    "    for word in File: \n",
    "        if word not in StopContents:\n",
    "            if word not in Dictionary:  # First time a word is added to dictionary\n",
    "                Dictionary[word] = {}\n",
    "                Dictionary[word][File_Name] = 1  # Setting term frequency for the document to 1\n",
    "            else:\n",
    "                if File_Name not in Dictionary[word]:\n",
    "                    Dictionary[word][File_Name] = 1  # Setting term frequency for the document to 1\n",
    "                else:\n",
    "                    Dictionary[word][File_Name] += 1  # Incrementing term frequency\n",
    "    \n",
    "    return Dictionary\n",
    "\n",
    "\n",
    "def BuildDocumentVectors():\n",
    "    # Function to build TF-IDF vectors for each document\n",
    "    \n",
    "    for Index, Key in enumerate(Dictionary): \n",
    "        for DocKeys in DocVectors.keys(): \n",
    "            if DocKeys in Dictionary[Key]:\n",
    "                DocFreq = len(Dictionary[Key]) \n",
    "                InvertedDocFreq = round(math.log(len(DocVectors) / DocFreq, 10), 2)  # Calculating IDF\n",
    "                TfIdf = InvertedDocFreq * Dictionary[Key][DocKeys]  # Calculating TF-IDF\n",
    "                DocVectors[DocKeys][Index] = TfIdf\n",
    "\n",
    "\n",
    "def QueryProcessor(Query):\n",
    "    # Function to process query text and convert it into a vector\n",
    "    \n",
    "    Query = Query.split()\n",
    "    Query = QueryStemmer(Query)  # Stemming query words\n",
    "    QueryVector = [0] * len(Dictionary)  # Initializing query vector\n",
    "    QueryDict = {}\n",
    "    \n",
    "    # Building dictionary for query\n",
    "    for word in Query: \n",
    "        if word not in QueryDict:\n",
    "            QueryDict[word] = 1\n",
    "        else:\n",
    "            QueryDict[word] += 1\n",
    "    \n",
    "    # Calculating TF-IDF for query vector\n",
    "    for Index, Key in enumerate(Dictionary): \n",
    "        if Key in QueryDict:\n",
    "            DocFreq = len(Dictionary[Key])\n",
    "            InvertedDocFreq = math.log(len(DocVectors) / DocFreq, 10)\n",
    "            TfIdf = InvertedDocFreq * QueryDict[Key]\n",
    "            QueryVector[Index] = TfIdf\n",
    "    \n",
    "    return QueryVector\n",
    "\n",
    "\n",
    "def QueryStemmer(Query):\n",
    "    # Function to stem words in query\n",
    "    \n",
    "    StemQuery = []\n",
    "    Stop = open('Stopword-List.txt', 'r')\n",
    "    StopContents = Stop.read()\n",
    "    StopContents = StopContents.split()\n",
    "    Stemmer = PorterStemmer()\n",
    "    Query = [Val for Val in Query if Val not in StopContents]\n",
    "    \n",
    "    # Stemming query words\n",
    "    for word in Query:\n",
    "        StemQuery.append(Stemmer.stem(word))\n",
    "    \n",
    "    return StemQuery  \n",
    "\n",
    "\n",
    "def EucDist(Vector):\n",
    "    # Function to calculate Euclidean distance for a vector\n",
    "    \n",
    "    Sum = 0\n",
    "    for i in Vector:\n",
    "        Sum += i ** 2\n",
    "    return math.sqrt(Sum)\n",
    "\n",
    "\n",
    "def Solver(Query,threshold):\n",
    "    # Function to solve the query and retrieve relevant documents\n",
    "    \n",
    "    ResultList = []\n",
    "    QueryEucDist = EucDist(Query)  # Calculating Euclidean length for the query\n",
    "    \n",
    "    if QueryEucDist == 0: \n",
    "        return ResultList  # Return empty list if the query vector is all zeros\n",
    "    \n",
    "    for Doc in DocVectors.keys():\n",
    "        Cosine = 0\n",
    "        DotProduct = 0\n",
    "        DocEucDist = EucDist(DocVectors[Doc])  # Calculating Euclidean length for a given document\n",
    "        \n",
    "        if DocEucDist == 0: \n",
    "            continue  # Skip calculation if the document vector is all zeros\n",
    "        \n",
    "        for i in range(0, len(Dictionary)):\n",
    "            if Query[i] == 0 or DocVectors[Doc][i] == 0: \n",
    "                continue  # Skip calculation if one of the TF-IDFs is zero\n",
    "            else:\n",
    "                DotProduct += Query[i] * DocVectors[Doc][i]\n",
    "        \n",
    "        Cosine = DotProduct / (QueryEucDist * DocEucDist)\n",
    "        \n",
    "        if Cosine > threshold:  # Threshold for example alpha = 0.05\n",
    "            ResultList.append((Doc, Cosine))\n",
    "    \n",
    "    # Sort results according to cosine value\n",
    "    ResultList = sorted(ResultList, key=lambda x:-x[1])  \n",
    "    return ResultList\n",
    "\n",
    "# Reading files, building dictionaries, and constructing document vectors\n",
    "Dictionary = FileRead()\n",
    "BuildDocumentVectors()\n",
    "\n",
    "threshold = float(input(\"Enter your desired threshold : \"))\n",
    "# threshold = 0.05\n",
    "# Query processing and solving loop\n",
    "while True:\n",
    "    Query = input(\"Enter Query (Type '-1' to exit): \")\n",
    "    if Query == '-1':\n",
    "        break\n",
    "    Query = QueryProcessor(str(Query))\n",
    "    print(Solver(Query,threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab969620-8e77-48d2-a6b0-ec266440108e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import string\n",
    "from nltk.stem import PorterStemmer\n",
    "import math \n",
    "import json\n",
    "import tkinter as tk\n",
    "import re \n",
    "import string\n",
    "from tkinter import messagebox\n",
    "Dictionary = {} #Create a global dictionary\n",
    "DocVectors = {} #Create a global dictionary for Document Vectors\n",
    "\n",
    "def load_data():\n",
    "    flag = False\n",
    "    if os.path.exists('Dict.json'):\n",
    "        flag = True\n",
    "        print(\"reading Dict\")\n",
    "        with open('Dict.json', 'r') as f:\n",
    "            Dictionary.update(json.load(f))\n",
    "    if os.path.exists('TFIDFVec.json'):\n",
    "        flag = True\n",
    "        print(\"reading TFIDFVec\")\n",
    "        with open('TFIDFVec.json', 'r') as f:\n",
    "            DocVectors.update(json.load(f))\n",
    "    return flag\n",
    "\n",
    "\n",
    "\n",
    "def FileRead(): \n",
    "    Folder = 'ResearchPapers'\n",
    "    Pattern = '*.txt' \n",
    "    FList = glob.glob(os.path.join(Folder, Pattern)) #Finding all Files in the given Folder \n",
    "    for Path in FList: \n",
    "        with open(Path, 'r') as file: \n",
    "            FileContents = file.read() #Reading File text\n",
    "            FileContents = FileContents.lower()\n",
    "            File_name = Path.strip(\"ResearchPapers\\\\.txt\")\n",
    "            FileContents = PunctuationRemove(FileContents)# Removing Punctuations\n",
    "            FileContents = FileContents.split() # Tokenizing string\n",
    "            Stemmer = PorterStemmer()\n",
    "            FileStem = []\n",
    "            #Applying Stemming to all the tokens\n",
    "            for words in list(FileContents):\n",
    "                FileStem.append(Stemmer.stem(words))\n",
    "            File_name = int(File_name)\n",
    "            Dictionary = DictionaryBuilder(FileStem,File_name)\n",
    "            Dictionary = sorted(Dictionary.items()) # Sorting the Dictionary by tokens\n",
    "            Dictionary = dict(Dictionary)\n",
    "    with open('Dict.json', 'w') as f:\n",
    "        json.dump(Dictionary, f)\n",
    "    # Initializing all Document Vectors with 0 for every word\n",
    "    for i in range(1,27):\n",
    "        if (i == 4 or i==5 or i==6 or i==10 or i==19 or i==20):\n",
    "            continue\n",
    "        DocVectors[i] = [0] * len(Dictionary)\n",
    "\n",
    "\n",
    "    return Dictionary\n",
    "\n",
    "\n",
    "def PunctuationRemove(File):\n",
    "    File = File.replace('-', ' ') # Replacing hyphens with spaces\n",
    "    File = File.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    File = re.sub(r'https?://(?:www\\.)?(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', File)  # remove urls\n",
    "    File = re.sub(r'\\S+\\.com\\b', '', File)  # remove .com\n",
    "    File = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', '', File)  # remove emails\n",
    "    File = re.sub(r'[^\\w\\s]', '', File)  # remove other useless punctuation\n",
    "    return File\n",
    "\n",
    "def DictionaryBuilder(File,File_Name):\n",
    "    Stop = open(r'Stopword-List.txt', 'r')\n",
    "    StopContents = Stop.read()\n",
    "    StopContents = StopContents.split()\n",
    "    for words in File: # Building Dictionary\n",
    "        if(words not in StopContents):\n",
    "            if(words not in Dictionary): # First time a word is added to Dictionary\n",
    "                Dictionary[words] = {}\n",
    "                Dictionary[words][File_Name] = 1 # Setting Term Frequency for the document to 1\n",
    "            else:\n",
    "                if(File_Name not in Dictionary[words]):\n",
    "                    Dictionary[words][File_Name] = 1 # Setting Term Frequency for the document to 1\n",
    "                else:\n",
    "                    Dictionary[words][File_Name] += 1 # Incrementing Term Frequency\n",
    "    return Dictionary   \n",
    "\n",
    "def BuildDocumentVectors():\n",
    "    for Index, Key in enumerate(Dictionary): # Traversing through words in Dictionary\n",
    "        for DocKeys in DocVectors.keys(): # Traversing through all Documents\n",
    "            if(DocKeys in Dictionary[Key]):\n",
    "                DocFreq = len(Dictionary[Key]) \n",
    "                InvertedDocFreq = round(math.log(len(DocVectors) / DocFreq, 10),2) # Calculating Inverted Document Frequency\n",
    "                TfIdf = InvertedDocFreq * Dictionary[Key][DocKeys] \n",
    "                #TfIdf = InvertedDocFreq * (1 + math.log(Dictionary[Key][DocKeys],10))\n",
    "                DocVectors[DocKeys][Index] = TfIdf\n",
    "    \n",
    "    with open('TFIDFVec.json', 'w') as f:\n",
    "        json.dump(DocVectors, f)\n",
    "\n",
    "def QueryProcessor(Query):\n",
    "    Query = Query.split()\n",
    "    Query = QueryStemmer(Query)\n",
    "    print(Query,end='\\n')\n",
    "    QueryVector = [0] * len(Dictionary) # Initializing Query Vector\n",
    "    QueryDict = {}\n",
    "    for words in Query: # Building Dictionary for Query\n",
    "        if(words not in QueryDict): # First time a word is added to Dictionary\n",
    "            QueryDict[words] = 1\n",
    "        else:\n",
    "            QueryDict[words] += 1\n",
    "    for Index, Key in enumerate(Dictionary): # Traversing Dictionary\n",
    "            if(Key in QueryDict):\n",
    "                DocFreq = len(Dictionary[Key])\n",
    "                InvertedDocFreq = math.log(len(DocVectors) / DocFreq, 10)\n",
    "                TfIdf = InvertedDocFreq * QueryDict[Key]\n",
    "                QueryVector[Index] = TfIdf\n",
    "    return QueryVector\n",
    "\n",
    "\n",
    "def QueryStemmer(Query):\n",
    "    StemQuery = []\n",
    "    Stop = open(r'Stopword-List.txt', 'r')\n",
    "    StopContents = Stop.read()\n",
    "    StopContents = StopContents.split()\n",
    "    Stemmer = PorterStemmer()\n",
    "    Query = [Val for Val in Query if Val not in StopContents]\n",
    "    for words in Query:\n",
    "        StemQuery.append(Stemmer.stem(words))\n",
    "    return StemQuery  \n",
    "\n",
    "# Calculating Eucilidean Length for a Vector\n",
    "def EucDist(Vector):\n",
    "    Sum = 0\n",
    "    for i in Vector:\n",
    "        Sum += i ** 2\n",
    "    return(math.sqrt(Sum))\n",
    "\n",
    "def Solver(Query):\n",
    "    ResultList = []\n",
    "    QueryEucDist = EucDist(Query) # Calculating Eucilidean Length for the Query\n",
    "    if(QueryEucDist == 0): # Return empty list if none of the words in the Query are in the Dictionary\n",
    "        return ResultList\n",
    "    for Doc in DocVectors.keys():\n",
    "        Cosine = 0\n",
    "        DotProduct = 0\n",
    "        DocEucDist = EucDist(DocVectors[Doc]) # Calculating Eucilidean Length for a given Document\n",
    "        if(DocEucDist == 0): # Return empty list if none of the words in the doc are in the Dictionary\n",
    "            continue\n",
    "        for i in range(0,len(Dictionary)):\n",
    "            if(Query[i] == 0 or DocVectors[Doc][i] == 0): # Skip calculation if one of the TF-IDFs are 0\n",
    "                continue\n",
    "            else:\n",
    "                DotProduct = DotProduct + (Query[i] * DocVectors[Doc][i])\n",
    "        Cosine = DotProduct / (QueryEucDist * DocEucDist)\n",
    "        if(Cosine > 0.01): # Threshold\n",
    "            ResultList.append((Doc,Cosine))\n",
    "    ResultList = sorted(ResultList, key=lambda x:-x[1]) # Sort results according to Cosine value\n",
    "    return ResultList\n",
    "\n",
    "\n",
    "def search_query():\n",
    "    query = entry_query.get()\n",
    "    query_vector = QueryProcessor(query)\n",
    "    results = Solver(query_vector)\n",
    "\n",
    "    if not results:\n",
    "        result_label.config(text=\"No documents match the query.\", fg=\"red\")\n",
    "    else:\n",
    "        result_label.config(text=\"Search Results sorted by cosine similarity:\", fg=\"black\")\n",
    "        result_text.delete('1.0', tk.END)\n",
    "        for result in results:\n",
    "            result_text.insert(tk.END, f\"Document ID: {result[0]}, Cosine Similarity: {result[1]}\\n\")\n",
    "\n",
    "\n",
    "# Check if the JSON files exist and load data\n",
    "sv= load_data()\n",
    "\n",
    "print(\"Dictionary and DocVectors already exist?\", sv)\n",
    "if sv is False:\n",
    "    Dictionary = FileRead()\n",
    "    BuildDocumentVectors()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "root = tk.Tk()\n",
    "root.title(\"Document Search Engine\")\n",
    "root.configure(bg=\"lavender\")  # Change background color here\n",
    "# Create label and entry for query input\n",
    "label_query = tk.Label(root, text=\"Enter your query:\", font=(\"Arial\", 12))\n",
    "label_query.grid(row=0, column=0, padx=10, pady=10, sticky=\"w\")\n",
    "entry_query = tk.Entry(root, width=50, font=(\"Arial\", 12))\n",
    "entry_query.grid(row=0, column=1, padx=10, pady=10)\n",
    "\n",
    "# Create search button\n",
    "button_search = tk.Button(root, text=\"Search\", command=search_query, font=(\"Arial\", 12))\n",
    "button_search.grid(row=0, column=2, padx=10, pady=10)\n",
    "\n",
    "# Create label and text widget for displaying search results\n",
    "result_label = tk.Label(root, text=\"\", font=(\"Arial\", 12, \"bold\"))\n",
    "result_label.grid(row=1, column=0, columnspan=3, pady=(20, 10))\n",
    "\n",
    "result_text = tk.Text(root, height=10, width=60, font=(\"Arial\", 12))\n",
    "result_text.grid(row=2, column=0, columnspan=3, padx=10)\n",
    "\n",
    "# Run the GUI\n",
    "root.mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
