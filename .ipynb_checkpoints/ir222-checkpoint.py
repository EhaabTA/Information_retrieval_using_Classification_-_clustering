# # -*- coding: utf-8 -*-
# """ir222.ipynb

# Automatically generated by Colab.

# Original file is located at
#     https://colab.research.google.com/drive/11sIXjczdnDZSwr2pAWIAohN6aAcGeQnh
# """

# import tkinter as tk
# from tkinter import scrolledtext
# import math
# import string
# import os
# import glob
# from nltk.stem import PorterStemmer
# from nltk.tokenize import word_tokenize

# Dictionary = {}  # Contains term frequencies for each word in each document
# DocVectors = {}  # Contains TF-IDF vectors for each document

# def FileRead():
#     # Function to read files, preprocess text, and build the dictionary

#     Folder = 'ResearchPapers'
#     Pattern = '*.txt'
#     FList = glob.glob(os.path.join(Folder, Pattern)) #Finding all Files in the given Folder

#     for Path in FList:
#         with open(Path, 'r') as file:
#             FileContents = file.read() #Reading File text
#             FileContents = FileContents.lower()
#             File_name = Path.strip("ResearchPapers\\.txt")
#             FileContents = PunctuationRemove(FileContents)# Removing Punctuations
#             FileContents = FileContents.split() # Tokenizing string
#             Stemmer = PorterStemmer()
#             FileStem = []
#             #Applying Stemming to all the tokens
#             for words in list(FileContents):
#                 FileStem.append(Stemmer.stem(words))
#             File_name = int(File_name)
#             Dictionary = DictionaryBuilder(FileStem,File_name)
#             Dictionary = sorted(Dictionary.items()) # Sorting the Dictionary by tokens
#             Dictionary = dict(Dictionary)

#     # Initializing all Document Vectors with 0 for every word
#     for i in range(1,26):
#          DocVectors[i] = [0] * len(Dictionary)

#     return Dictionary

# def PunctuationRemove(File):
#     # Function to remove punctuation marks from text
#     File = File.replace('-', ' ')  # Replacing hyphens with spaces
#     File = File.translate(str.maketrans("", "", string.punctuation))
#     return File

# def DictionaryBuilder(File, File_Name):
#     # Function to build the dictionary with term frequencies

#     Stop = open('Stopword-List.txt', 'r')
#     StopContents = Stop.read()
#     StopContents = StopContents.split()

#     # Building dictionary
#     for word in File:
#         if word not in StopContents:
#             if word not in Dictionary:  # First time a word is added to dictionary
#                 Dictionary[word] = {}
#                 Dictionary[word][File_Name] = 1  # Setting term frequency for the document to 1
#             else:
#                 if File_Name not in Dictionary[word]:
#                     Dictionary[word][File_Name] = 1  # Setting term frequency for the document to 1
#                 else:
#                     Dictionary[word][File_Name] += 1  # Incrementing term frequency

#     return Dictionary

# def BuildDocumentVectors():
#     # Function to build TF-IDF vectors for each document

#     for Index, Key in enumerate(Dictionary):
#         for DocKeys in DocVectors.keys():
#             if DocKeys in Dictionary[Key]:
#                 DocFreq = len(Dictionary[Key])
#                 InvertedDocFreq = round(math.log(len(DocVectors) / DocFreq, 10), 2)  # Calculating IDF
#                 TfIdf = InvertedDocFreq * Dictionary[Key][DocKeys]  # Calculating TF-IDF
#                 DocVectors[DocKeys][Index] = TfIdf

# def QueryProcessor(Query):
#     # Function to process query text and convert it into a vector

#     Query = Query.split()
#     Query = QueryStemmer(Query)  # Stemming query words
#     QueryVector = [0] * len(Dictionary)  # Initializing query vector
#     QueryDict = {}

#     # Building dictionary for query
#     for word in Query:
#         if word not in QueryDict:
#             QueryDict[word] = 1
#         else:
#             QueryDict[word] += 1

#     # Calculating TF-IDF for query vector
#     for Index, Key in enumerate(Dictionary):
#         if Key in QueryDict:
#             DocFreq = len(Dictionary[Key])
#             InvertedDocFreq = math.log(len(DocVectors) / DocFreq, 10)
#             TfIdf = InvertedDocFreq * QueryDict[Key]
#             QueryVector[Index] = TfIdf

#     return QueryVector

# def QueryStemmer(Query):
#     # Function to stem words in query

#     StemQuery = []
#     Stop = open('Stopword-List.txt', 'r')
#     StopContents = Stop.read()
#     StopContents = StopContents.split()
#     Stemmer = PorterStemmer()
#     Query = [Val for Val in Query if Val not in StopContents]

#     # Stemming query words
#     for word in Query:
#         StemQuery.append(Stemmer.stem(word))

#     return StemQuery

# def EucDist(Vector):
#     # Function to calculate Euclidean distance for a vector

#     Sum = 0
#     for i in Vector:
#         Sum += i ** 2
#     return math.sqrt(Sum)

# def Solver(Query):
#     # Function to solve the query and retrieve relevant documents

#     ResultList = []
#     QueryEucDist = EucDist(Query)  # Calculating Euclidean length for the query

#     if QueryEucDist == 0:
#         return ResultList  # Return empty list if the query vector is all zeros

#     for Doc in DocVectors.keys():
#         Cosine = 0
#         DotProduct = 0
#         DocEucDist = EucDist(DocVectors[Doc])  # Calculating Euclidean length for a given document

#         if DocEucDist == 0:
#             continue  # Skip calculation if the document vector is all zeros

#         for i in range(0, len(Dictionary)):
#             if Query[i] == 0 or DocVectors[Doc][i] == 0:
#                 continue  # Skip calculation if one of the TF-IDFs is zero
#             else:
#                 DotProduct += Query[i] * DocVectors[Doc][i]

#         Cosine = DotProduct / (QueryEucDist * DocEucDist)

#         if Cosine > 0.05:  # Threshold
#             ResultList.append((Doc, Cosine))

#     # Sort results according to cosine value
#     ResultList = sorted(ResultList, key=lambda x:-x[1])
#     return ResultList

# def on_click():
#     # Function to handle query button click event

#     query_text = query_entry.get("1.0", tk.END).strip()
#     if query_text:
#         Query = QueryProcessor(query_text)
#         results = Solver(Query)
#         result_text.delete('1.0', tk.END)
#         for result in results:
#             result_text.insert(tk.END, f"Document: {result[0]}, Cosine Similarity: {result[1]}\n")

# # Reading files, building dictionaries, and constructing document vectors
# Dictionary = FileRead()
# BuildDocumentVectors()

# # Creating GUI using tkinter
# root = tk.Tk()
# root.title("Document Search Engine")

# root.configure(bg="#000000")

# query_frame = tk.Frame(root)
# query_frame.pack(pady=10)

# query_label = tk.Label(query_frame, text="Enter your Query:", bg="#000000", fg="#FFFFFF")  # Set text and background color
# query_label.pack(side=tk.LEFT)

# query_entry = tk.Text(query_frame, height=5, width=100, bg="#FFFFFF", fg="#000000")  # Set text and background color
# query_entry.pack(side=tk.LEFT)

# query_button = tk.Button(query_frame, text="Search query", command=on_click, bg="#FFFFFF", fg="#000000")  # Set button color
# query_button.pack(side=tk.LEFT)

# result_text = scrolledtext.ScrolledText(root, width=80, height=20, wrap=tk.WORD, bg="#000000", fg="#FFFFFF")  # Set text and background color
# result_text.pack(padx=10, pady=10)

# root.mainloop()


################################################ ONLY REFACTORED
import glob
import nltk
import os
import string
from nltk.stem import PorterStemmer
import re
import math 

Dictionary = {}  # Contains term frequencies for each word in each document
DocVectors = {}  # Contains TF-IDF vectors for each document

def FileRead(): 
    Folder = 'ResearchPapers'
    Pattern = '*.txt' 
    FList = glob.glob(os.path.join(Folder, Pattern)) #Finding all Files in the given Folder 
    for Path in FList: 
        with open(Path, 'r') as file: 
            FileContents = file.read() #Reading File text
            FileContents = FileContents.lower()
            File_name = Path.strip("ResearchPapers\\.txt")
            FileContents = PunctuationRemove(FileContents)# Removing Punctuations
            FileContents = FileContents.split() # Tokenizing string
            Stemmer = PorterStemmer()
            FileStem = []
            #Applying Stemming to all the tokens
            for words in list(FileContents):
                FileStem.append(Stemmer.stem(words))
            File_name = int(File_name)
            Dictionary = DictionaryBuilder(FileStem,File_name)
            Dictionary = sorted(Dictionary.items()) # Sorting the Dictionary by tokens
            Dictionary = dict(Dictionary)
    # Initializing all Document Vectors with 0 for every word
    for i in range(1,31):
         DocVectors[i] = [0] * len(Dictionary)
    return Dictionary


def PunctuationRemove(File):
    # Function to remove punctuation marks from text
    File = File.replace('-', ' ')  # Replacing hyphens with spaces
    File = File.translate(str.maketrans("", "", string.punctuation))
    return File


def DictionaryBuilder(File, File_Name):
    # Function to build the dictionary with term frequencies
    
    # Open stopword list
    Stop = open('Stopword-List.txt', 'r')
    StopContents = Stop.read()
    StopContents = StopContents.split()
    
    # Building dictionary
    for word in File: 
        if word not in StopContents:
            if word not in Dictionary:  # First time a word is added to dictionary
                Dictionary[word] = {}
                Dictionary[word][File_Name] = 1  # Setting term frequency for the document to 1
            else:
                if File_Name not in Dictionary[word]:
                    Dictionary[word][File_Name] = 1  # Setting term frequency for the document to 1
                else:
                    Dictionary[word][File_Name] += 1  # Incrementing term frequency
    
    return Dictionary


def BuildDocumentVectors():
    # Function to build TF-IDF vectors for each document
    
    for Index, Key in enumerate(Dictionary): 
        for DocKeys in DocVectors.keys(): 
            if DocKeys in Dictionary[Key]:
                DocFreq = len(Dictionary[Key]) 
                InvertedDocFreq = round(math.log(len(DocVectors) / DocFreq, 10), 2)  # Calculating IDF
                TfIdf = InvertedDocFreq * Dictionary[Key][DocKeys]  # Calculating TF-IDF
                DocVectors[DocKeys][Index] = TfIdf


def QueryProcessor(Query):
    # Function to process query text and convert it into a vector
    
    Query = Query.split()
    Query = QueryStemmer(Query)  # Stemming query words
    QueryVector = [0] * len(Dictionary)  # Initializing query vector
    QueryDict = {}
    
    # Building dictionary for query
    for word in Query: 
        if word not in QueryDict:
            QueryDict[word] = 1
        else:
            QueryDict[word] += 1
    
    # Calculating TF-IDF for query vector
    for Index, Key in enumerate(Dictionary): 
        if Key in QueryDict:
            DocFreq = len(Dictionary[Key])
            InvertedDocFreq = math.log(len(DocVectors) / DocFreq, 10)
            TfIdf = InvertedDocFreq * QueryDict[Key]
            QueryVector[Index] = TfIdf
    
    return QueryVector


def QueryStemmer(Query):
    # Function to stem words in query
    
    StemQuery = []
    Stop = open('Stopword-List.txt', 'r')
    StopContents = Stop.read()
    StopContents = StopContents.split()
    Stemmer = PorterStemmer()
    Query = [Val for Val in Query if Val not in StopContents]
    
    # Stemming query words
    for word in Query:
        StemQuery.append(Stemmer.stem(word))
    
    return StemQuery  


def EucDist(Vector):
    # Function to calculate Euclidean distance for a vector
    
    Sum = 0
    for i in Vector:
        Sum += i ** 2
    return math.sqrt(Sum)


def Solver(Query,threshold):
    # Function to solve the query and retrieve relevant documents
    
    ResultList = []
    QueryEucDist = EucDist(Query)  # Calculating Euclidean length for the query
    
    if QueryEucDist == 0: 
        return ResultList  # Return empty list if the query vector is all zeros
    
    for Doc in DocVectors.keys():
        Cosine = 0
        DotProduct = 0
        DocEucDist = EucDist(DocVectors[Doc])  # Calculating Euclidean length for a given document
        
        if DocEucDist == 0: 
            continue  # Skip calculation if the document vector is all zeros
        
        for i in range(0, len(Dictionary)):
            if Query[i] == 0 or DocVectors[Doc][i] == 0: 
                continue  # Skip calculation if one of the TF-IDFs is zero
            else:
                DotProduct += Query[i] * DocVectors[Doc][i]
        
        Cosine = DotProduct / (QueryEucDist * DocEucDist)
        
        if Cosine > threshold:  # Threshold for example alpha = 0.05
            ResultList.append((Doc, Cosine))
    
    # Sort results according to cosine value
    ResultList = sorted(ResultList, key=lambda x:-x[1])  
    return ResultList

# Reading files, building dictionaries, and constructing document vectors
Dictionary = FileRead()
BuildDocumentVectors()

threshold = float(input("Enter your desired threshold : "))
# threshold = 0.05
# Query processing and solving loop
while True:
    Query = input("Enter Query (Type '-1' to exit): ")
    if Query == '-1':
        break
    Query = QueryProcessor(str(Query))
    print(Solver(Query,threshold))