{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09d555c0-023c-425f-b90e-43579eae6670",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Stopword-List.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 165\u001b[0m\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m#reading stopwords from stopwords file\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m#read_stopwords function takes filename as an input and return the list of stopwords\u001b[39;00m\n\u001b[1;32m--> 165\u001b[0m     stopwords \u001b[38;5;241m=\u001b[39m read_stopwords(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStopword-List.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;66;03m#read_all_files takes the stopwords as a parameter and reads all the files in the folder with a .txt extension and call tokenize method\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     \n\u001b[0;32m    169\u001b[0m     \n\u001b[0;32m    170\u001b[0m     \u001b[38;5;66;03m#dictionary is sorted first on keys and then on document id's\u001b[39;00m\n\u001b[0;32m    171\u001b[0m     \u001b[38;5;66;03m#sorted_dictionary = sort_dictionary(dictionary)\u001b[39;00m\n\u001b[0;32m    172\u001b[0m     inverted_index \u001b[38;5;241m=\u001b[39m clean_and_generate_positions(docs_directory)\n",
      "Cell \u001b[1;32mIn[3], line 14\u001b[0m, in \u001b[0;36mread_stopwords\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_stopwords\u001b[39m(filename):\n\u001b[1;32m---> 14\u001b[0m     file_read \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(filename,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     15\u001b[0m     content \u001b[38;5;241m=\u001b[39m file_read\u001b[38;5;241m.\u001b[39mread();\n\u001b[0;32m     16\u001b[0m     file_read\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    308\u001b[0m     )\n\u001b[1;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Stopword-List.txt'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import string\n",
    "from nltk.stem import PorterStemmer\n",
    "#nltk.download('punkt')\n",
    "\n",
    "stopwords = []\n",
    "dictionary = {}\n",
    "\n",
    "\n",
    "def read_stopwords(filename):\n",
    "    file_read = open(filename,'r')\n",
    "    content = file_read.read();\n",
    "    file_read.close()\n",
    "    stopwords = content.split('\\n')\n",
    "    return stopwords    \n",
    "\n",
    "def tokenizer(fileContent):\n",
    "    fileContent = fileContent.lower()\n",
    "    fileContent = fileContent.replace(\"-\", \" \")  # replace '-' with simple space\n",
    "    fileContent = fileContent.replace(\"â€¢\", \" \")  # replace '.' with simple space\n",
    "    fileContent = re.sub(r'https?://(?:www\\.)?(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', fileContent)  # remove urls\n",
    "    fileContent = re.sub(r'\\S+\\.com\\b', '', fileContent)  # remove .com\n",
    "    fileContent = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', '', fileContent)  # remove emails\n",
    "    fileContent = re.sub(r'\\b\\d+\\b', '', fileContent)  # remove numbers\n",
    "    fileContent = re.sub(r'[^\\w\\s]', '', fileContent)  # remove other useless punctuation\n",
    "    words = re.split(r'\\s+|\\n+', fileContent)\n",
    "    stemmer = PorterStemmer()\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "    return words\n",
    "\n",
    "docs_directory = \"ResearchPapers\"\n",
    "\n",
    "def clean_and_generate_positions(docs_directory): #cleaning the terms and creating the inverted indexes\n",
    "    # Build inverted index\n",
    "    inverted_index = {}\n",
    "\n",
    "    file_pattern = '*.txt'\n",
    "\n",
    "    # Use glob to find all files that match the pattern in the folder\n",
    "    file_list = glob.glob(os.path.join(docs_directory, file_pattern))\n",
    "    \n",
    "    # Loop over each file in the list and read its contents\n",
    "    for file_path in file_list:\n",
    "        with open(file_path, 'r') as file:\n",
    "            document_content = file.read()\n",
    "            file_name = os.path.basename(file_path).split('.')[0]\n",
    "            # Preprocess the document\n",
    "            processed_words = tokenizer(document_content)\n",
    "            # Update inverted index\n",
    "            for position, term in enumerate(processed_words):\n",
    "                if term not in stopwords:\n",
    "                    if term not in inverted_index:\n",
    "                        inverted_index[term] = {int(file_name): [position]}\n",
    "                    else:\n",
    "                        if int(file_name) not in inverted_index[term]:\n",
    "                            inverted_index[term][int(file_name)] = [position]\n",
    "                        else:\n",
    "                            inverted_index[term][int(file_name)].append(position)\n",
    "    return inverted_index\n",
    "\n",
    "\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def boolean_and(query_terms, inverted_index):\n",
    "    print(\"\\nThe terms obtained by spliting th user query is\",query_terms)\n",
    "    result = None\n",
    "    common_docs = set()\n",
    "    for term in query_terms:\n",
    "        stemmed_term = stemmer.stem(term)\n",
    "        postings = set(inverted_index.get(stemmed_term, {}).keys())\n",
    "        if result is None:\n",
    "            result = postings\n",
    "        else:\n",
    "            # Take the intersection of postings based on document name only\n",
    "            common_docs = common_docs.union(postings)\n",
    "    # Convert the result to a sorted list by docID\n",
    "    sorted_result = sorted(list(common_docs), key=lambda x: (int(x.split('.')[0]) if '.' in x else int(x)) if isinstance(x, str) else x)\n",
    "\n",
    "    return sorted_result\n",
    "\n",
    "\n",
    "def boolean_or(query_terms, inverted_index):\n",
    "    result = set()\n",
    "    print(\"\\nThe terms obtained by spliting th user query is\",query_terms)\n",
    "    for term in query_terms:\n",
    "        stemmed_term = stemmer.stem(term)\n",
    "        postings = set(inverted_index.get(stemmed_term, []))\n",
    "        result.update(postings)\n",
    "\n",
    "    result = list(result)\n",
    "\n",
    "    sorted_result = sorted(result, key=lambda x: (int(x.split('.')[0]) if '.' in x else int(x)) if isinstance(x, str) else x)\n",
    "\n",
    "    return sorted_result\n",
    "\n",
    "def boolean_not(not_query_terms, inverted_index):\n",
    "    result = set()\n",
    "    print(\"The terms obtained by spliting th user query is\",not_query_terms)\n",
    "    # Find all documents\n",
    "    all_docs = set()\n",
    "    for postings in inverted_index.values():\n",
    "        all_docs.update(postings.keys())\n",
    "    \n",
    "    # Find documents containing any of the terms from the NOT query\n",
    "    not_docs = set()\n",
    "    for term in not_query_terms:\n",
    "        stemmed_term = stemmer.stem(term)\n",
    "        postings = set(inverted_index.get(stemmed_term, {}).keys())\n",
    "        not_docs.update(postings)\n",
    "    \n",
    "    # Find documents not containing any of the terms from the NOT query\n",
    "    for doc_id in all_docs:\n",
    "        if str(doc_id) + '.txt' not in not_docs and str(doc_id) + '.txt' != \"Stopword-List.txt\":\n",
    "            result.add(str(doc_id) + '.txt')\n",
    "    \n",
    "    # Sort the result by docID\n",
    "    sorted_result = sorted(result, key=lambda x: int(x.split('.')[0]) if '.' in x else x)\n",
    "\n",
    "    return sorted_result\n",
    "\n",
    "def process_boolean_query(user_query, inverted_index):\n",
    "    # Tokenize the user input into a list of terms\n",
    "    query_terms = user_query.split()\n",
    "\n",
    "    # Define lists to store subqueries and operators\n",
    "    subqueries = []\n",
    "    operators = []\n",
    "\n",
    "    # Split the query into subqueries and operators\n",
    "    for term in query_terms:\n",
    "        if term.upper() in {'AND', 'OR', 'NOT'}:\n",
    "            operators.append(term.upper())\n",
    "        else:\n",
    "            subqueries.append(term)\n",
    "\n",
    "    # Evaluate the subqueries based on operators\n",
    "    result = []\n",
    "\n",
    "    if not operators:\n",
    "        for term in subqueries:\n",
    "            result.extend(inverted_index.get(term, []))\n",
    "        return result\n",
    "\n",
    "    # Iterate through operators and apply boolean operations\n",
    "    for i, operator in enumerate(operators):\n",
    "        if operator == 'AND':\n",
    "            i = i + 2\n",
    "            result = boolean_and(subqueries[:i], inverted_index)\n",
    "        elif operator == 'OR':\n",
    "            result = boolean_or(subqueries, inverted_index)\n",
    "        elif operator == 'NOT':\n",
    "            result = boolean_not(subqueries, inverted_index)\n",
    "            result = [doc.replace('.txt', '') for doc in result]\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #reading stopwords from stopwords file\n",
    "    #read_stopwords function takes filename as an input and return the list of stopwords\n",
    "    stopwords = read_stopwords('Stopword-List.txt')\n",
    "\n",
    "    #read_all_files takes the stopwords as a parameter and reads all the files in the folder with a .txt extension and call tokenize method\n",
    "    \n",
    "    \n",
    "    #dictionary is sorted first on keys and then on document id's\n",
    "    #sorted_dictionary = sort_dictionary(dictionary)\n",
    "    inverted_index = clean_and_generate_positions(docs_directory)\n",
    "    # Printing the first 10 terms from the sorted dictionary\n",
    "    \"\"\"\n",
    " print(\"Inverted index : \")\n",
    "    for term in inverted_index.items():\n",
    "        print(term, \":\", postings)\n",
    "        count += 1\n",
    "        if count == 1000:\n",
    "            break \n",
    "            \n",
    "    # Example AND query\n",
    "    and_query = ['feature', 'selection','redundancy']\n",
    "    result_and = boolean_and(and_query, inverted_index)\n",
    "\n",
    "    # Print the sorted result\n",
    "    print(f\" AND Query Result: {result_and}\")\n",
    "    \n",
    "    \n",
    "    # Example OR query\n",
    "    or_query = ['transformer', 'model']\n",
    "    result_or = boolean_or(or_query, inverted_index)\n",
    "\n",
    "    print(f\"OR Query Result: {result_or}\")\n",
    "    \n",
    "    \n",
    "    #Example NOT query\n",
    "    not_query = ['cancer', 'feature']\n",
    "    result_not = boolean_not(not_query, inverted_index)\n",
    "\n",
    "    result_not = [doc.replace('.txt', '') for doc in result_not]\n",
    "    # Print the result without \"txt\" extension\n",
    "    print(f\"NOT Query Result: {result_not}\")\n",
    "    \n",
    "    \n",
    "     #taking the user query\n",
    "    user_query = input(\"Enter any query : \")\n",
    "    result = process_boolean_query(user_query, inverted_index)\n",
    "    print(f\"The inverted index if the :{user_query} is\",result)\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "    user_query = input(\"Enter any query : \")\n",
    "    result = process_boolean_query(user_query, inverted_index)\n",
    "    print(f\"The inverted index if the :{user_query} is\",result)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
