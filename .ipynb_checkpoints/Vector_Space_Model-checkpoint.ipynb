{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o-1vu7R0FlI9",
    "outputId": "8f3d7912-fea6-4624-abb7-400477a278f5"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_punctuations_and_numbers(text):\n",
    "    # Remove punctuations\n",
    "    text_no_punctuations = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    # Remove numbers\n",
    "    text_no_punctuations_numbers = re.sub(r'\\d', '', text_no_punctuations)\n",
    "\n",
    "    return text_no_punctuations_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dNGymzD0JOPI",
    "outputId": "6f86563e-5b1d-4034-c271-29df5439e133"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Thinkpad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 748
    },
    "id": "-Ykzrzz-JS_U",
    "outputId": "38e73b53-81fe-4201-8169-2b340181dbf5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: '1.txt', 2: '11.txt', 3: '12.txt', 4: '13.txt', 5: '14.txt', 6: '15.txt', 7: '16.txt', 8: '17.txt', 9: '18.txt', 10: '2.txt', 11: '21.txt', 12: '22.txt', 13: '23.txt', 14: '24.txt', 15: '25.txt', 16: '26.txt', 17: '3.txt', 18: '7.txt', 19: '8.txt', 20: '9.txt'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nimport os\\nimport glob\\nfrom nltk.tokenize import word_tokenize\\nimport string\\n\\nindex = {}\\ndocument_list = {}\\n\\n# Define the folder path of the ResearchPapers folder\\ndocs_folder = \"ResearchPapers\"\\n\\n# Use glob to find all files that match the pattern in the folder\\nfile_list = glob.glob(os.path.join(docs_folder, \\'*.txt\\'))\\n\\nfor file_path in file_list:\\n    # Extract the original file name without extension\\n    file_name = os.path.splitext(os.path.basename(file_path))[0]\\n    # Initialize an empty list to store the document content\\n    content = []\\n    # Attempt different encodings until successful\\n    for encoding in [\\'utf-8\\', \\'latin1\\', \\'utf-16\\', \\'utf-16-le\\', \\'utf-16-be\\', \\'iso-8859-1\\']:\\n        try:\\n            with open(file_path, \\'r\\', encoding=encoding) as file:\\n                content = file.read()\\n                break  # Break the loop if the file is successfully decoded\\n        except UnicodeDecodeError:\\n            continue  # Try the next encoding if decoding fails\\n    if content:\\n        # Tokenize the document content\\n        words = word_tokenize(content)\\n        # Remove punctuations from tokens\\n        words = [word for word in words if word not in string.punctuation]\\n        # Update document_list and index\\n        for position, term in enumerate(words):\\n            if term not in document_list:\\n                document_list[term] = {file_name: [position]}\\n            else:\\n                if file_name not in document_list[term]:\\n                    document_list[term][file_name] = [position]\\n                else:\\n                    document_list[term][file_name].append(position)\\n\\n                            \\n    '"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "index = {}\n",
    "document_list = {}\n",
    "i = 1  # Start index from 1 instead of 0\n",
    "\n",
    "docs_folder = \"ResearchPapers\"  # Folder path of the ResearchPapers folder\n",
    "\n",
    "for file_name in os.listdir(docs_folder):\n",
    "    file_path = os.path.join(docs_folder, file_name)\n",
    "    for encoding in ['utf-8', 'latin1', 'utf-16', 'utf-16-le', 'utf-16-be', 'iso-8859-1']:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding=encoding) as file:\n",
    "                content = file.read()\n",
    "                # Tokenize the document content\n",
    "                words = word_tokenize(content)\n",
    "                # Remove punctuations from tokens\n",
    "                words = [word for word in words if word not in string.punctuation]\n",
    "                # Update document_list and index\n",
    "                document_list[i] = file_name\n",
    "                index[i] = words\n",
    "                i += 1\n",
    "                break  # Break the loop if the file is successfully decoded\n",
    "        except UnicodeDecodeError:\n",
    "            continue  # Try the next encoding if decoding fails\n",
    "print(document_list)\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "import os\n",
    "import glob\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "index = {}\n",
    "document_list = {}\n",
    "\n",
    "# Define the folder path of the ResearchPapers folder\n",
    "docs_folder = \"ResearchPapers\"\n",
    "\n",
    "# Use glob to find all files that match the pattern in the folder\n",
    "file_list = glob.glob(os.path.join(docs_folder, '*.txt'))\n",
    "\n",
    "for file_path in file_list:\n",
    "    # Extract the original file name without extension\n",
    "    file_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "    # Initialize an empty list to store the document content\n",
    "    content = []\n",
    "    # Attempt different encodings until successful\n",
    "    for encoding in ['utf-8', 'latin1', 'utf-16', 'utf-16-le', 'utf-16-be', 'iso-8859-1']:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding=encoding) as file:\n",
    "                content = file.read()\n",
    "                break  # Break the loop if the file is successfully decoded\n",
    "        except UnicodeDecodeError:\n",
    "            continue  # Try the next encoding if decoding fails\n",
    "    if content:\n",
    "        # Tokenize the document content\n",
    "        words = word_tokenize(content)\n",
    "        # Remove punctuations from tokens\n",
    "        words = [word for word in words if word not in string.punctuation]\n",
    "        # Update document_list and index\n",
    "        for position, term in enumerate(words):\n",
    "            if term not in document_list:\n",
    "                document_list[term] = {file_name: [position]}\n",
    "            else:\n",
    "                if file_name not in document_list[term]:\n",
    "                    document_list[term][file_name] = [position]\n",
    "                else:\n",
    "                    document_list[term][file_name].append(position)\n",
    "\n",
    "                            \n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "id": "8RjL3zTIKb4F",
    "outputId": "8c25c35a-d16f-4146-949f-ace698ae5ca6"
   },
   "outputs": [],
   "source": [
    "stop_words_file = \"Stopword-List.txt\"  # File path of the stop words file\n",
    "with open(stop_words_file) as file:\n",
    "    file_content = file.read()\n",
    "stop_words = word_tokenize(file_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "id": "3KMi6z3KM1FD"
   },
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "modified_index = {}\n",
    "for filename,tokens in index.items():\n",
    "  modified_index[filename] = [ps.stem(token.lower()) for token in tokens if token not in string.ascii_letters and token not in stop_words]\n",
    "\n",
    "for filename,tokens in modified_index.items():\n",
    "  modified_index[filename] = [len(tokens),tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "id": "6uvso8IM2Q_V"
   },
   "outputs": [],
   "source": [
    "for filename,value in modified_index.items():\n",
    "  document_list[filename] = (document_list[filename],value[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: ('1.txt', 11205), 1: ('11.txt', 1168), 2: ('12.txt', 17051), 3: ('13.txt', 6622), 4: ('14.txt', 6622), 5: ('15.txt', 8640), 6: ('16.txt', 10769), 7: ('17.txt', 6803), 8: ('18.txt', 3745), 9: ('2.txt', 7228), 10: ('21.txt', 6604), 11: ('22.txt', 13918), 12: ('23.txt', 2368), 13: ('24.txt', 3859), 14: ('25.txt', 4095), 15: ('26.txt', 9119), 16: ('3.txt', 2988), 17: ('7.txt', 30590), 18: ('8.txt', 4991), 19: ('9.txt', 2604)}\n"
     ]
    }
   ],
   "source": [
    "print(document_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_index = {}\n",
    "# key = term, value = [doc frequency,{Docid : frequency}]\n",
    "\n",
    "for filename,tokens in modified_index.items():\n",
    "  for token in tokens[1]:\n",
    "    if token not in inverted_index.keys():\n",
    "      inverted_index[token] = [1,{filename : 1}]\n",
    "    else:\n",
    "      dic = inverted_index[token]\n",
    "      if filename in dic[1].keys():\n",
    "        dic[1][filename]+=1\n",
    "      else:\n",
    "        dic[0] += 1\n",
    "        dic[1][filename] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "# Function to calculate TF-IDF\n",
    "def calculate_tfidf(idf, total_terms_in_doc, term_freq):\n",
    "    tf = term_freq / total_terms_in_doc\n",
    "    return tf * idf\n",
    "\n",
    "total_docs = 20\n",
    "tfidf_data = {}  # Dictionary to store TF-IDF data for each term\n",
    "idf = {}\n",
    "for term, values in inverted_index.items():\n",
    "    df = values[0]  # Document frequency is stored at index 0 of values\n",
    "    idf[term] = math.log(total_docs / 1+df)\n",
    "    tfidf_list = []  # List to store TF-IDF values for each document\n",
    "\n",
    "    for docid, term_freq in values[1].items():\n",
    "        total_terms_in_doc = document_list[docid][1]  # Total terms in the document\n",
    "        tfidf = calculate_tfidf(idf[term], total_terms_in_doc, term_freq)\n",
    "        tfidf_list.append({'docid': docid, 'tf_in_doc': term_freq, 'tf_idf': tfidf})\n",
    "\n",
    "    # Convert tfidf_list into a DataFrame indexed by document IDs\n",
    "    tfidf_df = pd.DataFrame(tfidf_list).set_index('docid')\n",
    "    tfidf_data[term] = tfidf_df\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Function to calculate TF-IDF\n",
    "def calculate_tfidf(idf, total_terms_in_doc, term_freq):\n",
    "    tf = term_freq / total_terms_in_doc\n",
    "    return tf * idf\n",
    "\n",
    "total_docs = 20\n",
    "tfidf_data = {}  # Dictionary to store TF-IDF data for each term\n",
    "idf = {}\n",
    "\n",
    "for term, values in inverted_index.items():\n",
    "    df = values[0]  # Document frequency is stored at index 0 of values\n",
    "    idf[term] = math.log(total_docs / (1 + df))\n",
    "    \n",
    "    tfidf_list = []  # List to store TF-IDF values for each document\n",
    "\n",
    "    for docid, term_freq in values[1].items():\n",
    "        total_terms_in_doc = document_list[docid][1]  # Total terms in the document\n",
    "        tfidf = calculate_tfidf(idf[term], total_terms_in_doc, term_freq)\n",
    "        tfidf_list.append({'docid': docid, 'tf_in_doc': term_freq, 'tf_idf': tfidf})\n",
    "\n",
    "    tfidf_data[term] = tfidf_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "id": "e_r2bLJ5445l"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'modified_index = None\\nindex = None\\ninverted_index = None'"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''modified_index = None\n",
    "index = None\n",
    "inverted_index = None'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ebc_itA14uI2",
    "outputId": "0294bb69-ce60-4828-f7f3-8139dbcf4ca7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          learn    machin\n",
      "docid                    \n",
      "0      0.000398  0.000498\n",
      "3      0.000085  0.000064\n",
      "4      0.000085  0.000064\n",
      "5      0.000148  0.000134\n",
      "6      0.000638  0.000186\n",
      "7      0.000490  0.000279\n",
      "8      0.000123  0.000197\n",
      "9      0.000490  0.000335\n",
      "10     0.000186  0.000048\n",
      "11     0.000188  0.000038\n",
      "12     0.000022  0.000044\n",
      "13     0.000425  0.000628\n",
      "14     0.000050  0.000129\n",
      "15     0.000281  0.000104\n",
      "16     0.000446  0.000388\n",
      "17     0.000347  0.000300\n",
      "18     0.000452  0.000633\n",
      "19     0.000020  0.000000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nimport pandas as pd\\nfrom nltk.stem import PorterStemmer\\n\\nps = PorterStemmer()\\nquery = \"cancer for learning\"\\nquery_terms = [ps.stem(term) for term in query.split()]\\nquery_series = pd.Series(query_terms).value_counts()\\n\\n# Initialize an empty DataFrame to store TF-IDF scores for each document and term\\ncosine = pd.DataFrame()\\n\\n# Initialize an empty dictionary to store IDF scores for each term in the query\\nquery_idf = {}\\n\\n# Iterate over each term in the query\\nfor term in set(query_terms):\\n    # Check if the term exists in the idf dictionary\\n    if term in idf:\\n        query_idf[term] = idf[term] * query_series.loc[term]\\n    else:\\n        query_idf[term] = 0.0  # Set IDF to 0 if term not found in the idf dictionary\\n\\n    if term in tfidf_data.keys():\\n        # Retrieve the document IDs and TF-IDF scores for the term\\n        term_data = tfidf_data[term][\\'tf_idf\\'].to_frame()\\n        # Rename the \\'tf_idf\\' column to the term name\\n        term_data.rename(columns={\\'tf_idf\\': term}, inplace=True)\\n        # Merge the term data with the existing DataFrame based on the \\'docid\\'\\n        cosine = pd.merge(left=cosine, right=term_data, how=\\'outer\\', left_index=True, right_index=True)\\n    else:\\n        term_data = pd.DataFrame(columns=[term])\\n        cosine = pd.merge(left=cosine, right=term_data, how=\\'outer\\', left_index=True, right_index=True)\\n\\n# Fill NaN values with 0\\ncosine.fillna(0.0, inplace=True)\\n\\n# Print the DataFrame\\nprint(cosine)\\n'"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "ps = PorterStemmer()\n",
    "query = \"machine learning\"\n",
    "query_terms = [ps.stem(term) for term in query.split()]\n",
    "query_series = pd.Series(query_terms).value_counts()\n",
    "# Initialize an empty DataFrame to store TF-IDF scores for each document and term\n",
    "cosine = pd.DataFrame()\n",
    "query = {}\n",
    "# Iterate over each term in the query\n",
    "for term in set(query_terms):\n",
    "    # Check if the term exists in the tfidf_data dictionary\n",
    "    query[term] = idf[term] * query_series.loc[term]\n",
    "    if term in tfidf_data.keys():\n",
    "        # Retrieve the document IDs and TF-IDF scores for the term\n",
    "        term_data = pd.DataFrame(tfidf_data[term]).set_index('docid')['tf_idf'].to_frame()\n",
    "        # Rename the 'tf_idf' column to the term name\n",
    "        term_data.rename(columns={'tf_idf': term}, inplace=True)\n",
    "\n",
    "        # Merge the term data with the existing DataFrame based on the 'docid'\n",
    "    else:\n",
    "      term_data = pd.DataFrame(columns=[term])\n",
    "    cosine = pd.merge(left = cosine,right = term_data, how='outer', left_index=True, right_index=True)\n",
    "\n",
    "# Fill NaN values with 0\n",
    "cosine.fillna(0.0, inplace=True)\n",
    "\n",
    "\n",
    "# Print the DataFrame\n",
    "print(cosine)\n",
    "'''\n",
    "import pandas as pd\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "query = \"cancer for learning\"\n",
    "query_terms = [ps.stem(term) for term in query.split()]\n",
    "query_series = pd.Series(query_terms).value_counts()\n",
    "\n",
    "# Initialize an empty DataFrame to store TF-IDF scores for each document and term\n",
    "cosine = pd.DataFrame()\n",
    "\n",
    "# Initialize an empty dictionary to store IDF scores for each term in the query\n",
    "query_idf = {}\n",
    "\n",
    "# Iterate over each term in the query\n",
    "for term in set(query_terms):\n",
    "    # Check if the term exists in the idf dictionary\n",
    "    if term in idf:\n",
    "        query_idf[term] = idf[term] * query_series.loc[term]\n",
    "    else:\n",
    "        query_idf[term] = 0.0  # Set IDF to 0 if term not found in the idf dictionary\n",
    "\n",
    "    if term in tfidf_data.keys():\n",
    "        # Retrieve the document IDs and TF-IDF scores for the term\n",
    "        term_data = tfidf_data[term]['tf_idf'].to_frame()\n",
    "        # Rename the 'tf_idf' column to the term name\n",
    "        term_data.rename(columns={'tf_idf': term}, inplace=True)\n",
    "        # Merge the term data with the existing DataFrame based on the 'docid'\n",
    "        cosine = pd.merge(left=cosine, right=term_data, how='outer', left_index=True, right_index=True)\n",
    "    else:\n",
    "        term_data = pd.DataFrame(columns=[term])\n",
    "        cosine = pd.merge(left=cosine, right=term_data, how='outer', left_index=True, right_index=True)\n",
    "\n",
    "# Fill NaN values with 0\n",
    "cosine.fillna(0.0, inplace=True)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(cosine)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lPoKI6UYjr8k",
    "outputId": "e58abd72-2dfc-476b-d91f-266118f7eeac"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[167], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m cosine_similarity \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Compute magnitude (L2 norm) of the query vector\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m query_magnitude \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(np\u001b[38;5;241m.\u001b[39msum(np\u001b[38;5;241m.\u001b[39msquare(\u001b[38;5;28mlist\u001b[39m(query\u001b[38;5;241m.\u001b[39mvalues()))))\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Iterate over rows of the DataFrame\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, document_dict \u001b[38;5;129;01min\u001b[39;00m cosine\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# Compute dot product of the query vector and the document vector\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "cosine_similarity = []\n",
    "\n",
    "# Compute magnitude (L2 norm) of the query vector\n",
    "query_magnitude = np.sqrt(np.sum(np.square(list(query.values()))))\n",
    "\n",
    "# Iterate over rows of the DataFrame\n",
    "for key, document_dict in cosine.iterrows():\n",
    "    # Compute dot product of the query vector and the document vector\n",
    "    dot_product = 0\n",
    "    for term, tfidf_score in query.items():\n",
    "        dot_product += tfidf_score * document_dict[term]  # Multiply corresponding TF-IDF scores\n",
    "\n",
    "    # Compute magnitude (L2 norm) of the document vector\n",
    "    document_magnitude = np.sqrt(np.sum(np.square(document_dict.values)))\n",
    "\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    cosine_similarity.append((key,dot_product / (query_magnitude * document_magnitude)))\n",
    "\n",
    "# Sort cosine similarities in descending order\n",
    "cosine_similarity = [row for row in cosine_similarity if row[1] > 0.5]\n",
    "cosine_similarity.sort(reverse=True, key = lambda x : x[1])\n",
    "\n",
    "# Print the sorted cosine similarities\n",
    "print(cosine_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SmuxcfU_vNlI"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
